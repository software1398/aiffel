{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E28 - 이미지 이상탐지모델(GAN)\n",
    "### Anomaly Detection with GAN\n",
    "AnoGAN : https://arxiv.org/pdf/1703.05921.pdf\n",
    "Anomaly 데이터가 부족한 상황에서도 Unsupervised방식으로 Anomaly detection 모델 구현이 가능한다.  \n",
    "generator x = G(z)는 latent variable z와 이미지 x 사이의 어던 맵핑 함수가 아닐까?  \n",
    "residual loss, discrimination loss  \n",
    "generator가 정상적으로 잘 학습이 되었다면 residual loss는 최소화될 것이고  \n",
    "discriminator가 잘 학습되었다면 중간 레이어 단계에서도 차이가 최소화 되어야 할 것이다.  \n",
    "결론적으로 loss를 최소화하는 파라미터를 찾는 게 아니라 z를 찾아나가는 것이다.  \n",
    "EGBAD : z를 찾는 것은 많은 계산량을 요구한다. latent space을 직접 모델링하자.  \n",
    "generator G, discriminator D외에 G의 역함수와 같은 E가 존재한다.\n",
    "E는 이미지 x를 latent space로 인코딩하는 encoder  \n",
    "그래서 AnoGAN와 BiGAN의 encoder 개념을 결합한 GANomaly 모델이 나오게 된다.  \n",
    "### GANomaly\n",
    "https://arxiv.org/pdf/1805.06725.pdf  \n",
    "이 모델의 generator는 일반적인 GAN과 달리 encoder-decoder구조를 가진다.\n",
    "이미지 x을 입력하여 latent variable z를 추출한 후 generator한 다음 Encoder를 한다.  \n",
    "3가지의 loss가 필요하며 이를 최소화한다.  \n",
    "adversarial loss, contextual loss, encoder loss\n",
    "adversarial loss - discriminator가 normal 데이터에 최적화되도록, discriminator의 중간 레이어 f사용\n",
    "contextual loss - AnoGAN의 Residual loss, generator가 normal 데이터를 잘 복원해내도록\n",
    "encoder loss - generator의 encoder의 latent variable과 encoder의 latent variable이 근사하도록\n",
    "contextual loss에서 encoder-decoder보다는 u-net이 더 낫지 않을까  \n",
    "### Skip-GANomaly\n",
    "https://arxiv.org/pdf/1901.08954.pdf  \n",
    "generator에서 u-net을 사용하여 GANomaly보다 더 간결하면서 복원력이 우수한 모델이 되었다.  \n",
    "Adversarial loss, contextual loss, latent loss, 전체 loss를 사용  \n",
    "### 데이터셋 구성(Fashion-MNIST)\n",
    "학습 방법은 라벨링된 데이터에서 특정 라벨링 데이터를 빼고 학습시킨 후 테스트 데이터에 특정 라벨링 데이터(이상 데이터)를 포함하여 이상감지 성능을 평가한다.  \n",
    "이번 노드에서는 fashion MNIST에서 8번 라벨인 가방을 이상 데이터로 가정하고 학습을 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, precision_recall_curve, average_precision_score\n",
    "from scipy.interpolate import interp1d\n",
    "from inspect import signature\n",
    "from scipy.optimize import brentq\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from scipy.stats import norm\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# 이상감지 데이터셋 구축\n",
    "# fashion-MNIST는 1채널의 grayscale 데이터셋이므로 reshape이 필요하다 \n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = (train_data - 127.5) / 127.5\n",
    "test_data = (test_data - 127.5) / 127.5\n",
    "\n",
    "# 1channel data reshape\n",
    "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1).astype('float32')\n",
    "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1).astype('float32')\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(train_data[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이상 데이터인 8번 라벨인 가방 제외하기\n",
    "\n",
    "def set_labels(labels):\n",
    "    new_t_labels = []\n",
    "    for old_label in labels:\n",
    "        if old_label == 8:   # Bag:8\n",
    "            new_t_labels.append([0])  # Bag을 이상치로 처리\n",
    "        else:\n",
    "            new_t_labels.append([1])  # 그 외의 경우는 정상치\n",
    "             \n",
    "    return np.array(new_t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bol_train_labels = set_labels(train_labels)\n",
    "bol_test_labels = set_labels(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "normal_labels = []\n",
    "anomaly_data = []\n",
    "anomaly_labels = []\n",
    "for data, label in zip(train_data, bol_train_labels):\n",
    "    if label == 0:\n",
    "        anomaly_data.append(data)\n",
    "        anomaly_labels.append(label)\n",
    "    else:\n",
    "        normal_data.append(data)\n",
    "        normal_labels.append(label)\n",
    "        \n",
    "normal_data = np.array(normal_data)\n",
    "normal_labels = np.array(normal_labels)\n",
    "anomaly_data = np.array(anomaly_data)\n",
    "anomaly_labels = np.array(anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 28, 28, 1) (54000, 1)\n",
      "(6000, 28, 28, 1) (6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(normal_data.shape, normal_labels.shape)\n",
    "print(anomaly_data.shape, anomaly_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = normal_data\n",
    "bol_train_labels = normal_labels\n",
    "test_data = tf.concat([test_data, anomaly_data], 0)\n",
    "bol_test_labels = tf.concat([bol_test_labels, anomaly_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 28, 28, 1)\n",
      "(16000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54000, 1)\n",
      "(16000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(bol_train_labels.shape)\n",
    "print(bol_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋이 정확하게 구성되었는지 검증\n",
    "\n",
    "for label in bol_train_labels:\n",
    "    if label == 0:\n",
    "        print(label)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련데이터셋에는 라벨이 1인 데이터만 존재, 테스트데이터셋에는 라벨이 0과 1인 데이터가 존재\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, bol_train_labels))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_data, bol_test_labels))\n",
    "test_dataset = test_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in train_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]], shape=(8, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for data, label in test_dataset.take(1):\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델과 Loss 함수 구성\n",
    "Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv_layer = tf.keras.Sequential([\n",
    "            layers.Conv2D(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                          kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.LeakyReLU(0.2),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outputs = self.conv_layer(inputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_T_block(tf.keras.Model):\n",
    "    def __init__(self, num_filters):\n",
    "        super(Conv_T_block, self).__init__()\n",
    "        self.conv_T_layer = tf.keras.Sequential([\n",
    "            layers.Conv2DTranspose(num_filters, 3, strides=2, padding='same', use_bias=False,\n",
    "                                   kernel_initializer=tf.random_normal_initializer(0., 0.02)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, concat, training=False):\n",
    "        upsample = self.conv_T_layer(inputs)\n",
    "        outputs = tf.concat([upsample, concat], -1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    def __init__(self, num_output_channel=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(512) # 1\n",
    "        \n",
    "        self.decoder_4 = Conv_T_block(512) # 2\n",
    "        self.decoder_3 = Conv_T_block(256) # 4\n",
    "        self.decoder_2 = Conv_T_block(128) # 8\n",
    "        self.decoder_1 = Conv_T_block(64) # 16\n",
    "        \n",
    "        self.output_layer = layers.Conv2DTranspose(num_output_channel, 1, strides=2, padding='same', use_bias=False, # 32\n",
    "                                                   kernel_initializer=tf.random_normal_initializer(0., 0.02))\n",
    "                \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # gen\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        de_4 = self.decoder_4(center, en_4)\n",
    "        de_3 = self.decoder_3(de_4, en_3)\n",
    "        de_2 = self.decoder_2(de_3, en_2)\n",
    "        de_1 = self.decoder_1(de_2, en_1)\n",
    "        \n",
    "        outputs = self.output_layer(de_1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator  \n",
    "conv_block - sigmoid -> 0~1사이의 숫자를 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.encoder_1 = Conv_block(64) # 16\n",
    "        self.encoder_2 = Conv_block(128) # 8\n",
    "        self.encoder_3 = Conv_block(256) # 4\n",
    "        self.encoder_4 = Conv_block(512) # 2\n",
    "        \n",
    "        self.center = Conv_block(100) # 1\n",
    "        \n",
    "        self.outputs = layers.Conv2D(1, 3, strides=1, padding='same',\n",
    "                                     use_bias=False, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        en_1 = self.encoder_1(inputs) # dis\n",
    "        en_2 = self.encoder_2(en_1)\n",
    "        en_3 = self.encoder_3(en_2)\n",
    "        en_4 = self.encoder_4(en_3)\n",
    "        \n",
    "        center = self.center(en_4)\n",
    "        \n",
    "        outputs = self.outputs(center)\n",
    "        \n",
    "        return outputs, center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.keras.losses.MeanSquaredError()\n",
    "l1_loss = tf.keras.losses.MeanAbsoluteError()\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(pred_real, pred_fake):\n",
    "    real_loss = cross_entropy(tf.ones_like(pred_real), pred_real)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(pred_fake), pred_fake)\n",
    "    \n",
    "    total_dis_loss = (real_loss + fake_loss) * 0.5\n",
    "    \n",
    "    return total_dis_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator loss함수에는 skip-GANomaly의 loss함수가 사용된다.\n",
    "\n",
    "def generator_loss(real_output, fake_output, input_data, gen_data, latent_first, latent_sec):\n",
    "    w_adv = 1.\n",
    "    w_context = 40.\n",
    "    w_encoder = 1.\n",
    "    \n",
    "    adv_loss = cross_entropy(real_output, fake_output)\n",
    "    context_loss = l1_loss(input_data, gen_data)\n",
    "    encoder_loss = l2_loss(latent_first, latent_sec)\n",
    "    \n",
    "    total_gen_loss = w_adv * adv_loss + \\\n",
    "                     w_context * context_loss + \\\n",
    "                     w_encoder * encoder_loss\n",
    "    \n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer 설정\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습과 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(images, training=True)\n",
    "        \n",
    "        pred_real, feat_real = discriminator(images, training=True)\n",
    "        pred_fake, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(pred_real, pred_fake,\n",
    "                                  images, generated_images,\n",
    "                                  feat_real, feat_fake)\n",
    "\n",
    "        disc_loss = discriminator_loss(pred_real, pred_fake)        \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(os.getenv('HOME'),'aiffel/ganomaly_skip_no_norm/ckpt')\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Dimensions of inputs should match: shape[0] = [8,8,8,128] vs. shape[1] = [8,7,7,128] [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1e8241f16b46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-371603dc9179>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgen_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc_tape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mgenerated_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpred_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-55b8941a211c>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mde_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mde_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mde_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mde_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d005df008625>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, concat, training)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mupsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_T_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1652\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1653\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [8,8,8,128] vs. shape[1] = [8,7,7,128] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        steps += 1\n",
    "        gen_loss, disc_loss = train_step(images)\n",
    "        \n",
    "        if steps % 100 == 0:\n",
    "            print ('Steps : {}, \\t Total Gen Loss : {}, \\t Total Dis Loss : {}'.format(steps, gen_loss.numpy(), disc_loss.numpy()))\n",
    "        \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_path)\n",
    "        \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(test_dataset, set_lambda=0.9):\n",
    "    an_scores = []\n",
    "    gt_labels = []\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(test_dataset):\n",
    "        generated_images = generator(x_batch_train, training=True)\n",
    "        _, feat_real = discriminator(x_batch_train, training=True)\n",
    "        _, feat_fake = discriminator(generated_images, training=True)\n",
    "\n",
    "        generated_images, feat_real, feat_fake = generated_images.numpy(), feat_real.numpy(), feat_fake.numpy()        \n",
    "\n",
    "        rec = abs(x_batch_train - generated_images)\n",
    "        lat = (feat_real - feat_fake) ** 2\n",
    "\n",
    "        rec = tf.reduce_sum(rec, [1,2,3])\n",
    "        lat = tf.reduce_sum(lat, [1,2,3])\n",
    "        \n",
    "        error = (set_lambda * tf.cast(rec, tf.float32)) + ((1 - set_lambda) * tf.cast(lat, tf.float32))\n",
    "        \n",
    "        an_scores.append(error)\n",
    "        gt_labels.append(y_batch_train)\n",
    "        \n",
    "    an_scores = np.concatenate(an_scores, axis=0).reshape([-1])\n",
    "    gt_labels = np.concatenate(gt_labels, axis=0).reshape([-1])\n",
    "    \n",
    "    an_scores = (an_scores - np.amin(an_scores)) / (np.amax(an_scores) - np.amin(an_scores))\n",
    "    \n",
    "    return an_scores, gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_scores, gt_labels = _evaluate(test_dataset)\n",
    "\n",
    "print(len(an_scores), len(gt_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨에 따라 anomaly score의 분포가 다르게 나타난다.\n",
    "normal = []\n",
    "anormaly = []\n",
    "for score, label in zip(an_scores, gt_labels):\n",
    "    if label == 0:\n",
    "        anormaly.append(score)\n",
    "    else:\n",
    "        normal.append(score)\n",
    "\n",
    "normal = np.array(normal)\n",
    "print(normal.shape)\n",
    "anormaly = np.array(anormaly)\n",
    "print(anormaly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(normal, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.hist(anormaly, bins=np.linspace(0.0, 1.0, num=100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(normal, norm.pdf(normal, np.mean(normal), np.std(normal)), 'o')\n",
    "plt.plot(anormaly, norm.pdf(anormaly, np.mean(anormaly), np.std(anormaly)), 'o')\n",
    "\n",
    "print(np.mean(normal), np.mean(anormaly))\n",
    "print(np.std(normal), np.std(anormaly))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
