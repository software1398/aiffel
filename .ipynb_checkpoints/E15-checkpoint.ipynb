{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E15 - 단어 수준의 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36745</th>\n",
       "      <td>I'm sorry about this.</td>\n",
       "      <td>J'en suis désolée.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109106</th>\n",
       "      <td>Do not give in to those demands.</td>\n",
       "      <td>Ne cède pas à ces exigences.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149153</th>\n",
       "      <td>I left work early and went straight home.</td>\n",
       "      <td>J'ai quitté mon travail tôt et suis rentré dir...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25977</th>\n",
       "      <td>She was all smiles.</td>\n",
       "      <td>Elle était tout sourire.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72483</th>\n",
       "      <td>The water is really dirty.</td>\n",
       "      <td>L'eau est très sale.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              eng  \\\n",
       "36745                       I'm sorry about this.   \n",
       "109106           Do not give in to those demands.   \n",
       "149153  I left work early and went straight home.   \n",
       "25977                         She was all smiles.   \n",
       "72483                  The water is really dirty.   \n",
       "\n",
       "                                                      fra  \\\n",
       "36745                                  J'en suis désolée.   \n",
       "109106                       Ne cède pas à ces exigences.   \n",
       "149153  J'ai quitté mon travail tôt et suis rentré dir...   \n",
       "25977                            Elle était tout sourire.   \n",
       "72483                                L'eau est très sale.   \n",
       "\n",
       "                                                       cc  \n",
       "36745   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "109106  CC-BY 2.0 (France) Attribution: tatoeba.org #4...  \n",
       "149153  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  \n",
       "25977   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "72483   CC-BY 2.0 (France) Attribution: tatoeba.org #5...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23026</th>\n",
       "      <td>Birds are chirping.</td>\n",
       "      <td>Les oiseaux chantent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22907</th>\n",
       "      <td>Are they satisfied?</td>\n",
       "      <td>Sont-ils contentés ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24499</th>\n",
       "      <td>I love fairy tales.</td>\n",
       "      <td>J'adore les contes de fées.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52772</th>\n",
       "      <td>You are not our friend.</td>\n",
       "      <td>Tu n'es pas notre amie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40423</th>\n",
       "      <td>Did you all know that?</td>\n",
       "      <td>Est-ce que vous connaissiez tous ça ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           eng                                    fra\n",
       "23026      Birds are chirping.                  Les oiseaux chantent.\n",
       "22907      Are they satisfied?                   Sont-ils contentés ?\n",
       "24499      I love fairy tales.            J'adore les contes de fées.\n",
       "52772  You are not our friend.                Tu n'es pas notre amie.\n",
       "40423   Did you all know that?  Est-ce que vous connaissiez tous ça ?"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][20000:53000] # 3만3천개의 샘플 사용, 세 번째 열은 제외\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 구현\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '<start> '\n",
    "eos_token = ' <end>'\n",
    "\n",
    "def preprocess(cols, sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    sentence = re.sub(r\"([?.!,¿,'-])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", r\" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    if cols == 'fra':\n",
    "        sentence = sos_token + sentence + eos_token\n",
    "            \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21322</th>\n",
       "      <td>they will survive .</td>\n",
       "      <td>&lt;start&gt; elles survivront . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22115</th>\n",
       "      <td>what happens next ?</td>\n",
       "      <td>&lt;start&gt; que se passe - t - il ensuite ? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25907</th>\n",
       "      <td>she invited him in .</td>\n",
       "      <td>&lt;start&gt; elle lui demanda de bien vouloir entre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35675</th>\n",
       "      <td>i kept an eye on tom .</td>\n",
       "      <td>&lt;start&gt; j ' ai gardé un œil sur tom . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34075</th>\n",
       "      <td>did you just hit tom ?</td>\n",
       "      <td>&lt;start&gt; est - ce que tu viens juste de taper t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35549</th>\n",
       "      <td>i had nothing to say .</td>\n",
       "      <td>&lt;start&gt; je n ' avais rien à dire . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45699</th>\n",
       "      <td>what is the cat up to ?</td>\n",
       "      <td>&lt;start&gt; qu ' est - ce que le chat manigance ? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24346</th>\n",
       "      <td>i have two cousins .</td>\n",
       "      <td>&lt;start&gt; j ' ai deux cousines . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30776</th>\n",
       "      <td>is this permissible ?</td>\n",
       "      <td>&lt;start&gt; est - ce autorisé ? &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42352</th>\n",
       "      <td>i need you to go home .</td>\n",
       "      <td>&lt;start&gt; j ' ai besoin que vous alliez à la mai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           eng  \\\n",
       "21322      they will survive .   \n",
       "22115      what happens next ?   \n",
       "25907     she invited him in .   \n",
       "35675   i kept an eye on tom .   \n",
       "34075   did you just hit tom ?   \n",
       "35549   i had nothing to say .   \n",
       "45699  what is the cat up to ?   \n",
       "24346     i have two cousins .   \n",
       "30776    is this permissible ?   \n",
       "42352  i need you to go home .   \n",
       "\n",
       "                                                     fra  \n",
       "21322                   <start> elles survivront . <end>  \n",
       "22115      <start> que se passe - t - il ensuite ? <end>  \n",
       "25907  <start> elle lui demanda de bien vouloir entre...  \n",
       "35675        <start> j ' ai gardé un œil sur tom . <end>  \n",
       "34075  <start> est - ce que tu viens juste de taper t...  \n",
       "35549           <start> je n ' avais rien à dire . <end>  \n",
       "45699  <start> qu ' est - ce que le chat manigance ? ...  \n",
       "24346               <start> j ' ai deux cousines . <end>  \n",
       "30776                  <start> est - ce autorisé ? <end>  \n",
       "42352  <start> j ' ai besoin que vous alliez à la mai...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.eng = lines.eng.apply(lambda x: preprocess('eng', x))\n",
    "lines.fra = lines.fra.apply(lambda x: preprocess('fra', x))\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 단어장을 만들기, 영어와 프랑스어별로\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = Tokenizer(filters=' ')            # 단어 단위로 Tokenizer를 생성합니다.\n",
    "    tokenizer.fit_on_texts(corpus)                # 33000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 2, 22, 70, 30, 174, 1],\n",
       " [3, 2, 22, 70, 30, 174, 1],\n",
       " [3, 2, 22, 70, 509, 1]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text, eng_tokenizer = tokenize(lines.eng)\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 6, 20, 4, 29, 31, 202, 10, 318, 3, 2],\n",
       " [1, 6, 20, 4, 29, 31, 202, 10, 634, 3, 2],\n",
       " [1, 6, 13, 130, 21, 436, 3, 2]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_text, fra_tokenizer = tokenize(lines.fra)\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng = lines.eng.apply(lambda x: x.split())\n",
    "# eng_tokenizer = Tokenizer()                          \n",
    "# eng_tokenizer.fit_on_texts(eng) \n",
    "# input_text = eng_tokenizer.texts_to_sequences(eng)    \n",
    "# input_text[:3]\n",
    "# # print(eng_tokenizer.word_index) 각 단어의 인덱스 확인하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fra = lines.fra.apply(lambda x: x.split())\n",
    "# fra_tokenizer = Tokenizer()                         # 단어 단위로 Tokenizer를 생성합니다. \n",
    "# fra_tokenizer.fit_on_texts(fra)                     # 33000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "# target_text = fra_tokenizer.texts_to_sequences(fra) # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "# target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 5211\n",
      "프랑스어 단어장의 크기 : 9021\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 11\n",
      "프랑스어 시퀀스의 최대 길이 20\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)\n",
    "# 패딩처리하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 33000\n",
      "영어 단어장의 크기 : 5211\n",
      "프랑스어 단어장의 크기 : 9021\n",
      "영어 시퀀스의 최대 길이 11\n",
      "프랑스어 시퀀스의 최대 길이 20\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [x[:-1] for x in target_text]\n",
    "# 시작 토큰 제거\n",
    "decoder_target = [x[1:] for x in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 6, 20, 4, 29, 31, 202, 10, 318, 3], [1, 6, 20, 4, 29, 31, 202, 10, 634, 3], [1, 6, 13, 130, 21, 436, 3]]\n",
      "[[6, 20, 4, 29, 31, 202, 10, 318, 3, 2], [6, 20, 4, 29, 31, 202, 10, 634, 3, 2], [6, 13, 130, 21, 436, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sent in decoder_target:\n",
    "#     print(' '.join(list(map(lambda x: fra_tokenizer.index_word[x], sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (33000, 11)\n",
      "프랑스어 입력데이터의 크기(shape) : (33000, 20)\n",
      "프랑스어 출력데이터의 크기(shape) : (33000, 20)\n"
     ]
    }
   ],
   "source": [
    "# 패딩 처리하기\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (30000, 11)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (30000, 20)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (30000, 20)\n"
     ]
    }
   ],
   "source": [
    "# 학습데이터와 검증데이터 분리하기\n",
    "\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input_train))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input_train))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Masking, Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "embedding_size = 1024\n",
    "hidden_size = 512\n",
    "\n",
    "# 인코더 설계\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(eng_vocab_size, embedding_size)(encoder_inputs)\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb)\n",
    "encoder_lstm = LSTM(hidden_size, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb =  Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking, initial_state=encoder_states)\n",
    "\n",
    "# 출력층\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 1024)   5336064     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 1024)   9237504     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 1024)   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 1024)   0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 3147776     masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  3147776     masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9021)   4627773     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 25,496,893\n",
      "Trainable params: 25,496,893\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "118/118 [==============================] - 19s 161ms/step - loss: 1.9555 - val_loss: 1.6680\n",
      "Epoch 2/30\n",
      "118/118 [==============================] - 19s 159ms/step - loss: 1.2888 - val_loss: 1.3897\n",
      "Epoch 3/30\n",
      "118/118 [==============================] - 20s 166ms/step - loss: 1.0748 - val_loss: 1.2305\n",
      "Epoch 4/30\n",
      "118/118 [==============================] - 20s 166ms/step - loss: 0.9392 - val_loss: 1.1612\n",
      "Epoch 5/30\n",
      "118/118 [==============================] - 20s 167ms/step - loss: 0.8327 - val_loss: 1.0856\n",
      "Epoch 6/30\n",
      "118/118 [==============================] - 19s 161ms/step - loss: 0.7439 - val_loss: 1.0503\n",
      "Epoch 7/30\n",
      "118/118 [==============================] - 18s 152ms/step - loss: 0.6657 - val_loss: 0.9957\n",
      "Epoch 8/30\n",
      "118/118 [==============================] - 18s 152ms/step - loss: 0.5948 - val_loss: 0.9674\n",
      "Epoch 9/30\n",
      "118/118 [==============================] - 18s 153ms/step - loss: 0.5322 - val_loss: 0.9501\n",
      "Epoch 10/30\n",
      "118/118 [==============================] - 18s 151ms/step - loss: 0.4761 - val_loss: 0.9213\n",
      "Epoch 11/30\n",
      "118/118 [==============================] - 18s 152ms/step - loss: 0.4257 - val_loss: 0.9154\n",
      "Epoch 12/30\n",
      "118/118 [==============================] - 19s 158ms/step - loss: 0.3802 - val_loss: 0.8960\n",
      "Epoch 13/30\n",
      "118/118 [==============================] - 18s 156ms/step - loss: 0.3397 - val_loss: 0.8948\n",
      "Epoch 14/30\n",
      "118/118 [==============================] - 18s 152ms/step - loss: 0.3035 - val_loss: 0.8865\n",
      "Epoch 15/30\n",
      "118/118 [==============================] - 18s 156ms/step - loss: 0.2720 - val_loss: 0.8883\n",
      "Epoch 16/30\n",
      "118/118 [==============================] - 19s 159ms/step - loss: 0.2439 - val_loss: 0.8859\n",
      "Epoch 17/30\n",
      "118/118 [==============================] - 19s 164ms/step - loss: 0.2199 - val_loss: 0.8815\n",
      "Epoch 18/30\n",
      "118/118 [==============================] - 18s 155ms/step - loss: 0.1980 - val_loss: 0.8852\n",
      "Epoch 19/30\n",
      "118/118 [==============================] - 18s 154ms/step - loss: 0.1793 - val_loss: 0.8906\n",
      "Epoch 20/30\n",
      "118/118 [==============================] - 19s 158ms/step - loss: 0.1636 - val_loss: 0.8979\n",
      "Epoch 21/30\n",
      "118/118 [==============================] - 19s 159ms/step - loss: 0.1494 - val_loss: 0.9021\n",
      "Epoch 22/30\n",
      "118/118 [==============================] - 19s 158ms/step - loss: 0.1378 - val_loss: 0.9025\n",
      "Epoch 23/30\n",
      "118/118 [==============================] - 18s 155ms/step - loss: 0.1281 - val_loss: 0.9104\n",
      "Epoch 24/30\n",
      "118/118 [==============================] - 18s 153ms/step - loss: 0.1198 - val_loss: 0.9125\n",
      "Epoch 25/30\n",
      "118/118 [==============================] - 18s 154ms/step - loss: 0.1122 - val_loss: 0.9136\n",
      "Epoch 26/30\n",
      "118/118 [==============================] - 18s 155ms/step - loss: 0.1058 - val_loss: 0.9271\n",
      "Epoch 27/30\n",
      "118/118 [==============================] - 19s 159ms/step - loss: 0.1002 - val_loss: 0.9245\n",
      "Epoch 28/30\n",
      "118/118 [==============================] - 19s 161ms/step - loss: 0.0956 - val_loss: 0.9285\n",
      "Epoch 29/30\n",
      "118/118 [==============================] - 18s 156ms/step - loss: 0.0917 - val_loss: 0.9389\n",
      "Epoch 30/30\n",
      "118/118 [==============================] - 19s 157ms/step - loss: 0.0882 - val_loss: 0.9477\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train,\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=256, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='traingin loss', color='red')\n",
    "plt.plot(epochs, val_loss, 'b', label='validation loss', color='blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, None, 1024)        5336064   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, None, 1024)        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 512), (None, 512) 3147776   \n",
      "=================================================================\n",
      "Total params: 8,483,840\n",
      "Trainable params: 8,483,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 테스트하기\n",
    "\n",
    "# 인코더 정의\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 1024)   9237504     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  3147776     embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9021)   4627773     lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 17,013,053\n",
      "Trainable params: 17,013,053\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더 정의\n",
    "\n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2 =  Embedding(fra_vocab_size, embedding_size)(decoder_inputs)\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_outputs2)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs2] + decoder_states2)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수->단어, 단어->정수\n",
    "\n",
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_sequence(): 입력 인자는 번역하고자 하는 문장의 정수 시퀀스\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = fra2idx['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' ' + sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<end>' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: he owns a lot of land .\n",
      "정답 문장: il possède beaucoup de terre .\n",
      "번역기가 번역한 문장: il beaucoup un un d\n",
      "-----------------------------------\n",
      "입력 문장: they seem surprised .\n",
      "정답 문장: ils semblent surpris .\n",
      "번역기가 번역한 문장: elles l l l les de d\n",
      "-----------------------------------\n",
      "입력 문장: don ' t spoil your child .\n",
      "정답 문장: ne gâtez pas votre enfant .\n",
      "번역기가 번역한 문장: ne ne ne de ici ! ! \n",
      "-----------------------------------\n",
      "입력 문장: you lack imagination .\n",
      "정답 문장: vous manquez d ' imagination .\n",
      "번역기가 번역한 문장: tu vous - . . . . . \n",
      "-----------------------------------\n",
      "입력 문장: she taught us singing .\n",
      "정답 문장: elle nous enseigna à chanter .\n",
      "번역기가 번역한 문장: elle nous nous nous nou\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "sel_lst = [rd.randint(1, 30000) for _ in range(5)]\n",
    "\n",
    "for seq_index in sel_lst:\n",
    "    input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index + 20000])\n",
    "    print('정답 문장:', lines.fra[seq_index + 20000][8:-6])\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
