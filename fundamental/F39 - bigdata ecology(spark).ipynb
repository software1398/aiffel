{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F39 - 빅데이터 생태계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### . 2002년   \n",
    "    더그 커팅(Doug Cutting)과 마이크 카파렐라(Mike Cafarellr)가 웹 크롤러 너치(Nutch)개발   \n",
    "###### . 2003년   \n",
    "    구글에서 분산 처리용 파일 시스템 발표 GFS(Google File System)   \n",
    "###### . 2004년   \n",
    "    구글에서 분산 처리용 프로그래밍 모델 발표 맵리듀스(MapReduce)   \n",
    "###### . 2005년   \n",
    "    더그 커팅, 마이크 카파렐라 주도 하에 NDFS(Nutch Distributed File System) 프로젝트 시작   \n",
    "###### . 2006년   \n",
    "    더그 커팅 야후 합류   \n",
    "    야후에서 최소의 하둡 기반 클러스터 구축   \n",
    "    구글에서 빅테이블 공개, 이후 아파치 하둡의 HBase 근간이 된다.   \n",
    "###### . 2007년   \n",
    "    아파치 하둡 1.0 발표   \n",
    "###### . 2008년   \n",
    "    하둡의 하위 프로젝트로 SQL관련 컴포넌트 개발 하이브(Hive)   \n",
    "    클라우데라 창업 (하둡 상용화 버전)   \n",
    "###### . 2009년   \n",
    "    UC 버클리대학의 마태 자하리아가 스파크 연구 시작   \n",
    "###### . 2012년   \n",
    "    하둡의 컴포넌트인 얀(YARN)개발   \n",
    "    최초의 하둡용 네이티브 MPP 분석용 컴포넌트 임팔라(Impala)개발   \n",
    "###### . 2014년   \n",
    "    아파치 스파크 1.0출시   \n",
    "    마태 자하리아, 알리 고디시 등 데이터브릭스 창업"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark 설치하기   \n",
    "java, spark, python 필요   \n",
    "sudo apt-get install openjdk-8-jdk-headless -y   자바설치하기   \n",
    "\n",
    "spark 다운받기\n",
    "wget http://mirror.apache-kr.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz\n",
    "tar xf spark-3.0.1-bin-hadoop2.7.tgz   압축풀기\n",
    "cd spark-3.0.1-bin-hadoop2.7\n",
    "bin폴더 내에 ./spark-shell 실행\n",
    "\n",
    "pyspark 설치 pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.12:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkContext객체를 통해서만 spark의 모든 기능에 접근이 가능하다.\n",
    "# 스파크 프로그램당 한번만 실행 가능, 사용 후에는 종료해야 한다.\n",
    "# SparkContext객체 생성 = spark 초기화\n",
    "# pyspark의 sparkContext는 JVM위에서 동작하는 Py4J의 sparkContext와 연결된다.\n",
    "# sparkContext의 주요 내용\n",
    "# 1. 문법: pyspark.SparkContext()\n",
    "# 2. 스파크기능의 기본 엔트리 포인트\n",
    "# 3. 스파크 클러스터와 연결을 나타내며 RDD를 만들고 브로드캐스트하는데 사용될 수 있다.\n",
    "# 4. JVM당 하나만 활성화해야 하며, 새로운 것을 만들기 전에 활성을 중지해야 한다.\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-ecceb6144d92>:13 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3a8b76a558aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sparkContext()객체를 더 만들 시 에러 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnew_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    339\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 341\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-ecceb6144d92>:13 "
     ]
    }
   ],
   "source": [
    "# sparkContext()객체를 더 만들 시 에러 발생\n",
    "\n",
    "new_sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종료\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.12:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 설정 세팅 가능\n",
    "sc = SparkContext(master='local', appName='PySpark Basic')\n",
    "sc\n",
    "\n",
    "# spark web-ui버전 이용시 로컬 모드의 경우 http://localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.port', '45789'),\n",
       " ('spark.driver.host', '192.168.0.12'),\n",
       " ('spark.app.id', 'local-1600693574122'),\n",
       " ('spark.app.name', 'PySpark Basic'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 생성한 sparkcontext객체의 설정확인\n",
    "\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PySpark Basic'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sc.master)\n",
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb9b7a191bd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PySpark Basic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName('PySpark Basic').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD creation\n",
    "# 1. parallelize(): 내부에서 만들어진 데이터 집합을 병렬화하는 방법\n",
    "\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아직 RDD는 생성되지 않았으며 계보만 만들어놓은 상태, actions 실행 시 생성된다.\n",
    "# take(): 원소를 반환, 인자는 RDD의 원소의 갯수\n",
    "\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# 2. .textfile(): 외부의 파일을 로드하는 방법\n",
    "\n",
    "import os\n",
    "\n",
    "file_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/test.txt'\n",
    "with open(file_path, 'w') as f:\n",
    "    for i in range(10):\n",
    "        f.write(str(i)+'\\n')\n",
    "        \n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiffel0042/aiffel/bigdata_ecosystem/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# 외부에서 파일을 읽어올 때 데이터 타입은 string이다.\n",
    "rdd2 = sc.textFile(file_path)\n",
    "print(rdd2)\n",
    "print(type(rdd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c', 'd']\n",
      "[('b', 1), ('a', 1), ('c', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "# RDD Operation\n",
    "# 1. transformations\n",
    "# map()\n",
    "\n",
    "x = sc.parallelize([\"b\", \"a\", \"c\", \"d\"])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect()) #collect()는 actions입니다. \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3])\n",
    "squares = nums.map(lambda x: x*x)\n",
    "print(squares.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "# filter()\n",
    "\n",
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = x.filter(lambda x: x%2 == 0) \n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "text = sc.parallelize(['a', 'b', 'c', 'd'])\n",
    "capital = text.map(lambda x: x.upper())\n",
    "A = capital.filter(lambda x: 'A' in x)\n",
    "print(text.collect())\n",
    "print(A.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 10, 30, 2, 20, 30, 3, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "# flatmap()\n",
    "\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, x*10, 30))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'is',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'by',\n",
       " 'python']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsDataset = sc.parallelize([\"Spark is funny\", \"It is beautiful\", \"And also It is implemented by python\"])\n",
    "result = wordsDataset.flatMap(lambda x: x.split()).filter(lambda x: x != \" \").map(lambda x: x.lower())\n",
    "# 공백은 제거합니다.\n",
    "# 단어를 공백기준으로 split 합니다. \n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone',\n",
       " '0,male,22.0,1,0,7.25,Third,unknown,Southampton,n',\n",
       " '1,female,38.0,1,0,71.2833,First,C,Cherbourg,n',\n",
       " '1,female,26.0,0,0,7.925,Third,unknown,Southampton,y',\n",
       " '1,female,35.0,1,0,53.1,First,C,Southampton,n']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV 파일 읽어들이기\n",
    "\n",
    "import os\n",
    "csv_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_0.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 비어있는 라인은 제외하고, delimeter인 ,로 line을 분리해 줍니다. \n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "csv_data_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = csv_data_1.take(1)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '28.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8.4583',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Queenstown',\n",
       "  'y']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())  # 첫번째 컬럼이 숫자인 것만 필터링\n",
    "csv_data_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '35.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '53.1'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '28.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '8.4583'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Queenstown'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "csv_data_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class|deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|0       |male  |22.0|1                 |0    |7.25   |Third|unknown|Southampton|n    |\n",
      "|1       |female|38.0|1                 |0    |71.2833|First|C      |Cherbourg  |n    |\n",
      "|1       |female|26.0|0                 |0    |7.925  |Third|unknown|Southampton|y    |\n",
      "|1       |female|35.0|1                 |0    |53.1   |First|C      |Southampton|n    |\n",
      "|0       |male  |28.0|0                 |0    |8.4583 |Third|unknown|Queenstown |y    |\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# csv파일을 df형태로 읽어들이기\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "url = 'https://storage.googleapis.com/tf-datasets/titanic/train.csv'\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# sqlContext의 read.csv()함수를 이용하여 df로 만들기(판다스의 df랑 유사하다)\n",
    "# sqlContext: https://spark.apache.org/docs/1.6.1/sql-programming-guide.html\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"train.csv\"), header=True, inferSchema= True)\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class |deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|0       |male  |66.0|0                 |0    |10.5   |Second|unknown|Southampton|y    |\n",
      "|0       |male  |42.0|1                 |0    |52.0   |First |unknown|Southampton|n    |\n",
      "|1       |female|49.0|1                 |0    |76.7292|First |D      |Cherbourg  |n    |\n",
      "|0       |male  |65.0|0                 |1    |61.9792|First |B      |Cherbourg  |n    |\n",
      "|0       |male  |45.0|1                 |0    |83.475 |First |C      |Southampton|n    |\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 위에서 얻은 데이터에서 40세 이상인 사람들의 데이터만 필터링해 봅시다. \n",
    "df2 = df[df['age']>40]\n",
    "df2.show(5, truncate = False)\n",
    "\n",
    "# transformations: https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect(): RDD내 모든 값을 리턴한다.\n",
    "\n",
    "nums = sc.parallelize(list(range(10)))\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count(): RDD내 데이터 갯수\n",
    "nums.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce()\n",
    "\n",
    "nums.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxr-xr-x 2 aiffel0042 aiffel0042  4096  9월 22 09:49 file.txt\r\n",
      "-rw-r--r-- 1 aiffel0042 aiffel0042    20  9월 22 09:49 test.txt\r\n",
      "-rw-r--r-- 1 aiffel0042 aiffel0042 30874  2월 21  2019 train.csv\r\n"
     ]
    }
   ],
   "source": [
    "# saveAsTextFile(): RDD 데이터를 저장한다.\n",
    "\n",
    "file_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/file.txt'\n",
    "nums.saveAsTextFile(file_path)\n",
    "\n",
    "!ls -l ~/aiffel/bigdata_ecosystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actions: https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions\n",
    "\n",
    "# RDD 생성\n",
    "rdd = sc.parallelize(range(1,100))\n",
    "\n",
    "# RDD Transformation \n",
    "rdd2 = rdd.map(lambda x: 0.5*x - 10).filter(lambda x: x > 0)\n",
    "\n",
    "# RDD Action \n",
    "rdd2.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 1),\n",
       " ('h', 2),\n",
       " ('o', 2),\n",
       " ('l', 2),\n",
       " ('y', 1),\n",
       " ('e', 1),\n",
       " ('t', 1),\n",
       " ('p', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word counter 구현하기\n",
    "\n",
    "text = sc.parallelize('hello python')\n",
    "\n",
    "# map 함수를 적용한 RDD 구하기\n",
    "rdd = text.filter(lambda x: x != \" \").map(lambda x: (x, 1))\n",
    "\n",
    "#reduceByKey 함수를 적용한 Word Counter 출력\n",
    "rdd2 = rdd.reduceByKey(lambda accum, n: accum + n)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Titanic 데이터 분석하기\n",
    "\n",
    "# 이전 스텝에서 CSV 파일을 로딩했던 내역입니다. \n",
    "csv_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "columns = csv_data_1.take(1)\n",
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())\n",
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "\n",
    "csv_data_3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생존자의 평균 연령: 29.110411522633743\n",
      "사망자의 평균 연령: 29.9609375\n"
     ]
    }
   ],
   "source": [
    "# csv_data_3를 가공하여 생존자, 사망자의 연령 총합과 사람 수를 각각 구해 봅시다. \n",
    "# 이후 각각의 데이터로부터 생존자와 사망자의 평균 연령을 구할 수 있습니다. \n",
    "\n",
    "titanic_map = csv_data_3.map(lambda x: (x[0][1], float(x[2][1])))\n",
    "age_sum = titanic_map.reduceByKey(lambda accum, n: accum + n).collect()\n",
    "\n",
    "cnt = csv_data_3.map(lambda x: (x[0][1], 1)).reduceByKey(lambda accum, n: accum + n).collect()\n",
    "\n",
    "age_sum_dict = dict(age_sum)\n",
    "cnt_dict = dict(cnt)\n",
    "avg_survived = age_sum_dict['1']/cnt_dict['1']\n",
    "avg_died = age_sum_dict['0']/cnt_dict['0']\n",
    "print('생존자의 평균 연령: {}'.format(avg_survived))\n",
    "print('사망자의 평균 연령: {}'.format(avg_died))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
