{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV17 - Human Pose Estimation with Keypoint detection\n",
    "https://darkpgmr.tistory.com/77\n",
    "https://darkpgmr.tistory.com/83?category=460965\n",
    "\n",
    "키포인트를 찾는 방법\n",
    "1. Top-down\n",
    "   object detection 사용하여 crop한 이미지 내에서 찾는다.  \n",
    "   사람이 많이 등장할 경우 느리다.  \n",
    "2. Bottom-up\n",
    "   모든 키포인트를 먼저 찾아서 사람의 형태로 맞추어 나간다.  \n",
    "   detect하지 않기 때문에 속도 저하가 없지만 모든 키포인트 찾아내기 때문에 성능이 떨어진다.  \n",
    "   \n",
    "등장 하는 사람의 수에 따라 방법을 정하면 된다.\n",
    "\n",
    "### human ketpoint detection\n",
    "https://github.com/Team-Neighborhood/Kalman-Filter-Image  \n",
    "키포인트 찾는데 어려운 점: invisible, occlusions, clothing, lighting change 등  \n",
    "https://nanonets.com/blog/human-pose-estimation-2d-guide/  \n",
    "인체는 단위 부분으로 나눌 수 있고 그 부분은 서로 연결성을 가지고 있다. 3D에서는 구분이 되지만 2D에서는 구분이 잘 안된다.  \n",
    "각 부분의 complex joint relationship의 mixture model로 키포인트를 표현한다.  \n",
    "https://www.cs.cmu.edu/~deva/papers/pose_pami.pdf\n",
    "\n",
    "DeepPose  \n",
    "Toshev and Szegedy가 처음으로 딥러닝 기반 keypoint localization 모델 제안  \n",
    "https://arxiv.org/pdf/1312.4659.pdf  \n",
    "초기의 pose estimation은 x, y좌표를 직접 예측하는 position regression문제로 인식하였다. (L2 loss)  \n",
    "\n",
    "Efficient object localization Using convolutional network  \n",
    "x, y좌표를 직접 예측하기보다는 keypoint가 존재할 확률 분포가 높은 곳을 찾자.  \n",
    "x, y좌표를 중심으로 한 heatmap를 학습하도록 한다. 엄청난 성능 향상을 가져온다.  \n",
    "https://arxiv.org/pdf/1411.4280.pdf  \n",
    "\n",
    "Convolutional Pose Machines  \n",
    "https://arxiv.org/pdf/1602.00134.pdf  \n",
    "이전에는 crop 연산 등으로 비연속적인 stage로 나뉘어져 있어서 학습 과정을 반복하는 단점이 있었다.  \n",
    "end-to-end, 한 번에 학습할 수 있다.\n",
    "stage1에서 image feature를 생성, stage2부터는 keypoint를 예측한다.  \n",
    "각 stage의 g1, g2 함수는 모두 heatmap를 출력하여 재사용하며 weight sharing을 하게 된다.  \n",
    "stage2을 반복하면 receptive field가 넓어지면서 joint를 재조정하면서 성능이 향상되었다.  \n",
    "\n",
    "Stacked Hourglass Network  \n",
    "feature map upsampling, residual connection이 주요점이다.  \n",
    "pooling으로 이미지의 global feature를 찾고 upsampling으로 local feature를 고려한다.  \n",
    "기본 구조가 hourglass형태이며 이를 여러 개 쌓는다.  \n",
    "MPII에서 처음으로 PCKh@0.5기준 90%이상 나옴.\n",
    "\n",
    "SimpleBaseline\n",
    "아주 간단한 encoder-decoder 구조, 73.7%의 mAP 성능이 나옴, resnet50만 사용한 간단한 구조이지만 hourglass를 넘겼다.  \n",
    "Deep High-Resolution Network(HRNet)  \n",
    "SOTA에 가까운 모델, SimpliBaseline과 같은 저자라 같은 철학을 보여줌.  \n",
    "multi-stage가 아닌 1stage구조를 고수하는 간단한 모델을 지향\n",
    "hourglass와 simplebaseline의 공통점과 차이점은?  \n",
    "encoder(high) -> low -> decoder(high) 과정에서 최초의 정보를 최종까지 얼마나 많이 효율적으로 가지고 갈 수 있을까?  \n",
    "down sample layer: 작아진 layer feature를 upsampling하여 원본 해상도 크기에 적용하는 방법  \n",
    "MSE loass 이용  \n",
    "https://github.com/leoxiaobin/deep-high-resolution-net.pytorch 원저작자 github\n",
    "\n",
    "### SimpleBaseline 구조 코드로 이해하기\n",
    "https://arxiv.org/pdf/1804.06208.pdf  \n",
    "encoder: resnet을 backbone  \n",
    "decoder: deconv - bn - relu, 256filter size, 4x4 kernel, stride 2  \n",
    "https://github.com/Microsoft/human-pose-estimation.pytorch  원저작자의 github repo\n",
    "모델 부분 : https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py  \n",
    "torch.nn == keras.layers  \n",
    "29번째 줄 basicblock == keras.modes: https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L29  \n",
    "forward함수: https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L42  \n",
    "4개의 residual block: https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L157  \n",
    "forward함수: https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L234  \n",
    "resnet - deconv_layers - final_layer  \n",
    "deconv layer: https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L219  \n",
    "파라미터 config 파일: https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/experiments/coco/resnet50/256x192_d256x3_adam_lr1e-3.yaml#L23  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplebaseline tf 버전\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv module\n",
    "upconv1 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn1 = tf.keras.layers.BatchNormalization()\n",
    "relu1 = tf.keras.layers.ReLU()\n",
    "upconv2 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn2 = tf.keras.layers.BatchNormalization()\n",
    "relu2 = tf.keras.layers.ReLU()\n",
    "upconv3 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn3 = tf.keras.layers.BatchNormalization()\n",
    "relu3 = tf.keras.layers.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv module 함수로 만들기\n",
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = tf.keras.layers.Conv2D(17, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256, 192, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Model)             multiple                  23587712  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 64, 48, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 48, 1)         257       \n",
      "=================================================================\n",
      "Total params: 34,077,569\n",
      "Trainable params: 34,022,913\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n",
      "(1, 256, 192, 3)\n",
      "(1, 64, 48, 1)\n",
      "tf.Tensor(\n",
      "[[[-2.37211911e-03]\n",
      "  [ 6.58433978e-03]\n",
      "  [-1.95678752e-02]\n",
      "  [-6.39829272e-03]\n",
      "  [-6.63550338e-03]\n",
      "  [-9.39821638e-03]\n",
      "  [ 3.42872227e-03]\n",
      "  [-7.97738321e-04]\n",
      "  [ 2.01119343e-03]\n",
      "  [ 3.12840356e-03]]\n",
      "\n",
      " [[ 6.31613599e-04]\n",
      "  [-9.90851130e-03]\n",
      "  [ 6.34088367e-03]\n",
      "  [-1.99240651e-02]\n",
      "  [-3.79849877e-03]\n",
      "  [ 2.27478631e-02]\n",
      "  [-3.07023190e-02]\n",
      "  [ 1.59196388e-02]\n",
      "  [-2.82666311e-02]\n",
      "  [-1.71598885e-03]]\n",
      "\n",
      " [[-2.03404371e-02]\n",
      "  [-9.92842857e-03]\n",
      "  [-1.47836655e-02]\n",
      "  [-6.64129248e-03]\n",
      "  [-3.91905196e-02]\n",
      "  [-2.50287168e-02]\n",
      "  [-3.72883491e-03]\n",
      "  [ 1.38006173e-03]\n",
      "  [-3.22503857e-02]\n",
      "  [ 9.73710697e-03]]\n",
      "\n",
      " [[ 2.74955272e-03]\n",
      "  [-4.05716039e-02]\n",
      "  [-2.90545244e-02]\n",
      "  [-2.54658125e-02]\n",
      "  [ 2.00015921e-02]\n",
      "  [-1.75925344e-02]\n",
      "  [-1.80871971e-02]\n",
      "  [ 3.78958546e-02]\n",
      "  [ 2.24218182e-02]\n",
      "  [-6.11622520e-02]]\n",
      "\n",
      " [[-3.11561693e-02]\n",
      "  [ 7.46092573e-03]\n",
      "  [-2.29152460e-02]\n",
      "  [ 6.30454579e-03]\n",
      "  [-7.27723911e-03]\n",
      "  [ 3.84420203e-03]\n",
      "  [-3.10024973e-02]\n",
      "  [ 1.43398568e-02]\n",
      "  [-6.13500783e-03]\n",
      "  [-1.39732938e-02]]\n",
      "\n",
      " [[-2.22269986e-02]\n",
      "  [-1.72356386e-02]\n",
      "  [ 1.18160862e-02]\n",
      "  [ 1.62879713e-02]\n",
      "  [-4.12070863e-02]\n",
      "  [-1.51784215e-02]\n",
      "  [-1.31105278e-02]\n",
      "  [ 1.10009406e-02]\n",
      "  [ 5.96245751e-04]\n",
      "  [-4.16513085e-02]]\n",
      "\n",
      " [[-9.71769914e-05]\n",
      "  [ 2.05364805e-02]\n",
      "  [-8.79498385e-03]\n",
      "  [-5.53157507e-03]\n",
      "  [-2.85401847e-02]\n",
      "  [ 1.39378328e-02]\n",
      "  [-2.31903698e-02]\n",
      "  [-8.09541345e-03]\n",
      "  [-4.99003083e-02]\n",
      "  [ 4.97572757e-02]]\n",
      "\n",
      " [[ 7.00229825e-03]\n",
      "  [ 7.93349557e-03]\n",
      "  [ 1.31340222e-02]\n",
      "  [-3.35185081e-02]\n",
      "  [-1.01347314e-02]\n",
      "  [-1.00190639e-02]\n",
      "  [-4.32183594e-02]\n",
      "  [-7.06527941e-03]\n",
      "  [-1.00673875e-02]\n",
      "  [-3.21238935e-02]]\n",
      "\n",
      " [[-1.64191574e-02]\n",
      "  [ 9.20405437e-04]\n",
      "  [-4.22765017e-02]\n",
      "  [-3.13290730e-02]\n",
      "  [ 2.17846129e-03]\n",
      "  [-1.04422253e-02]\n",
      "  [-6.10339083e-02]\n",
      "  [-6.25081360e-04]\n",
      "  [ 3.15331016e-03]\n",
      "  [-1.64123029e-02]]\n",
      "\n",
      " [[-5.58539759e-04]\n",
      "  [ 5.33045921e-03]\n",
      "  [ 9.54344124e-03]\n",
      "  [ 1.17947720e-02]\n",
      "  [-1.52773941e-02]\n",
      "  [ 4.34497520e-02]\n",
      "  [ 1.36104040e-03]\n",
      "  [ 5.23945987e-02]\n",
      "  [-2.32095085e-02]\n",
      "  [-3.43433693e-02]]], shape=(10, 10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(1, kernel_size=(1,1), padding='same')\n",
    "\n",
    "# input :  192x256\n",
    "# output : 48x64\n",
    "inputs = keras.Input(shape=(256, 192, 3))\n",
    "x = resnet(inputs)\n",
    "x = upconv(x)\n",
    "out = final_layer(x)\n",
    "model = keras.Model(inputs, out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# np_input = np.zeros((1,256,192,3), dtype=np.float32)\n",
    "np_input = np.random.randn(1,256,192,3)\n",
    "np_input = np.zeros((1,256,192,3), dtype=np.float32)\n",
    "tf_input = tf.convert_to_tensor(np_input, dtype=np.float32)\n",
    "print (tf_input.shape) # TensorShape([1,256,192,3])\n",
    "\n",
    "tf_output = model(tf_input)\n",
    "\n",
    "print (tf_output.shape)\n",
    "print (tf_output[0,:10,:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
