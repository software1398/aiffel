{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV16 - 불안한 시선 이펙트 추가하기\n",
    "mkdir -p ~/aiffel/coarse_to_fine/data  \n",
    "cd ~/aiffel/coarse_to_fine  \n",
    "wget https://aiffelstaticprd.blob.core.windows.net/media/documents/coarse_to_fine_pjt.zip  \n",
    "unzip coarse_to_fine_pjt.zip  \n",
    "### 위치 측정을 위한 라벨링 툴 만들기\n",
    "#### OpenCV 사용\n",
    "품질이 좋지 못한 데이터에 잘못 라벨링 되었을 경우 올바른 위치에 눈동자를 지정하여 fine라벨로 만들어야 한다.  \n",
    "이때 눈동자 위치를 선택할 수 있는 도구가 있어야 한다.  \n",
    "openCV에서는 마우스 이벤트를 callback함수 형태로 지원한다.  \n",
    "callback함수 : https://satisfactoryplace.tistory.com/18  \n",
    "openCV에서 지원하는 마우스 이벤트 형태 : https://opencv-python.readthedocs.io/en/latest/doc/04.drawWithMouse/drawWithMouse.html  \n",
    "#### 툴 만들기\n",
    "keypoint_using_mouse.py을 이용하여 눈동자에 마우스 클릭하여 s을 입력하여 좌표를 저장한다.  \n",
    "#### 데이터를 모아보자\n",
    "라벨링할 초기 데이터 수집, 직접 촬영하거나 수집해도 되지만 공개된 데이터를 이용하자.  \n",
    "눈동자 위치를 필요로 하기 때문에  \n",
    "1. 눈이 크롭되어 있고 눈동자 위치를 라벨로 가지고 있는 데이터  \n",
    "2. 얼굴 랜드마크를 가지고 있는 데이터  \n",
    "3. 얼굴 이미지를 가지고 있는 데이터  \n",
    "순으로 찾으면 좋겠다.  \n",
    "1번에 해당하는 데이터셋 - BioID : https://www.bioid.com/facedb/\n",
    "1521장은 너무 작다. 얼굴 랜드마크가 제공되는 데이터를 찾아보자. 랜드마크가 제공되니깐 눈 부분 크롭 가능  \n",
    "dlib은 얼굴 랜드마크를 사용하는데 어떤 학습 데이터셋을 사용했을 것이다. 그것을 찾아보자.  \n",
    "dlib face landmark dataset 구글 검색  \n",
    "http://dlib.net/face_landmark_detection.py.html 찾았다. iBUG 300-W라는 데이터셋으로 학습했다고...  \n",
    "운이 좋군. 하지만 실무에서는 대부분 관련 도메인 데이터셋이 없는 경우가 많다고 한다.  \n",
    "이를 경우 3번 경우처럼 얼굴 이미지 데이터셋에서 랜드마크를 추출하고 눈 부분을 크롭한 후 라벨링해야 한다.  \n",
    "LFW데이터셋은 안면 인식과 관련된 데이터셋으로 얼굴 이미지만 있다.  \n",
    "\n",
    "wget http://vis-www.cs.umass.edu/lfw/lfw.tgz  \n",
    "mv lfw.tgz ~/aiffel/coarse_to_fine/data  \n",
    "cd ~/aiffel/coarse_to_fine/data && tar -xvzf lfw.tgz  \n",
    "### Mean-shift를 이용한 눈동자 검출 방법\n",
    "#### 이론\n",
    "눈동자는 어떻게 찾을 수 있을까? 눈동자는 가운데 어두운 색을 가지고 있다. 색 반전 후 밝은 색, 최대값을 찾는 방법이 있다.  \n",
    "하지만 머리카락이 내려와 겹쳐있을 경우 눈의 가장자리 혹은 다른 곳을 찾게 될 수 있다.  \n",
    "2차원 블러 특성 이미지에서 눈동자에 2차원의 정규분포로 나타나는 영역이 보인다.  \n",
    "1차원 누적 그래프로 보면 x축으로 2개의 봉우리가 보인다. 가장자리부터 누적된 값을 확인하면 최대값인 곳에 도달할 것이다.  \n",
    "2D에서 최고점을 찾는 방법  \n",
    "1. 이미지 중심점을 초기값으로 설정: 눈의 중심에 눈동자가 있을 확률이 높기 때문에\n",
    "2. 중심점을 기준으로 작은 box을 설정: box의 크기는 상황에 따라 적절하게 설정\n",
    "3. box내부의 픽셀값을 이용해서 무게중심 찾기: pixel intensity를 weight로 사용 가능  \n",
    "4. 찾은 무게중심을 기준을 box의 중심으로 설정: 박스가 이동\n",
    "5. 이동한 지점에서 위 과정을 반복\n",
    "6. 수렴할 때까지 수 번을 반복하면 눈동자 찾을 수 있다.  \n",
    "머신러닝에 Mean Shift라는 알고리즘이 이런 이론이다.  \n",
    "탐색반경 내 데이터 포인트의 평균을 구하고 그 평균 위치로 이동, 이 과정을 반복하면서 데이터 분포의 중심으로 이동  \n",
    "영상추적 - mean shift 추적 : https://darkpgmr.tistory.com/64  \n",
    "\n",
    "#### 실습\n",
    "cd ~/aiffel/coarse_to_fine && python eye_center_basic.py True  \n",
    "True옵션은 매 스텝의 작동을 확인 가능, False나 생략하면 최종 결과만 확인 가능  \n",
    "cd ~/aiffel/coarse_to_fine && cp eye_center_basic.py eye_center_meanshift.py  \n",
    "eye_center_meanshift.py에서 findCenterPoint함수를 다음과 같이 수정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCenterPoint(gray_eye, str_direction='left'):\n",
    "    if gray_eye is None:\n",
    "        return [0, 0]\n",
    "    filtered_eye = cv2.bilateralFilter(gray_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "\n",
    "    # 2D images -> 1D signals\n",
    "    row_sum = 255 - np.sum(filtered_eye, axis=0)//gray_eye.shape[0]\n",
    "    col_sum = 255 - np.sum(filtered_eye, axis=1)//gray_eye.shape[1]\n",
    "\n",
    "    # normalization & stabilization\n",
    "    def vector_normalization(vector):\n",
    "        vector = vector.astype(np.float32)\n",
    "        vector = (vector-vector.min())/(vector.max()-vector.min()+1e-6)*255\n",
    "        vector = vector.astype(np.uint8)\n",
    "        vector = cv2.blur(vector, (5,1)).reshape((vector.shape[0],))\n",
    "        vector = cv2.blur(vector, (5,1)).reshape((vector.shape[0],))            \n",
    "        return vector\n",
    "    row_sum = vector_normalization(row_sum)\n",
    "    col_sum = vector_normalization(col_sum)\n",
    "\n",
    "    def findOptimalCenter(gray_eye, vector, str_axis='x'):\n",
    "        axis = 1 if str_axis == 'x' else 0\n",
    "        center_from_start = np.argmax(vector)\n",
    "        center_from_end = gray_eye.shape[axis]-1 - np.argmax(np.flip(vector,axis=0))\n",
    "        return (center_from_end + center_from_start) // 2\n",
    "\n",
    "        # x 축 center 를 찾는 알고리즘을 mean shift 로 대체합니다.\n",
    "    # center_x = findOptimalCenter(gray_eye, row_sum, 'x')\n",
    "    center_y = findOptimalCenter(gray_eye, col_sum, 'y')\n",
    "\n",
    "        # 수정된 부분\n",
    "    inv_eye = (255 - filtered_eye).astype(np.float32)\n",
    "    inv_eye = (255*(inv_eye - inv_eye.min())/(inv_eye.max()-inv_eye.min())).astype(np.uint8)\n",
    "\n",
    "    resized_inv_eye = cv2.resize(inv_eye, (inv_eye.shape[1]//3, inv_eye.shape[0]//3))\n",
    "    init_point = np.unravel_index(np.argmax(resized_inv_eye),resized_inv_eye.shape)\n",
    "\n",
    "    x_candidate = init_point[1]*3 + 1\n",
    "    for idx in range(10):\n",
    "        temp_sum = row_sum[x_candidate-2:x_candidate+3].sum()\n",
    "        if temp_sum == 0:\n",
    "            break\n",
    "        normalized_row_sum_part = row_sum[x_candidate-2:x_candidate+3].astype(np.float32)//temp_sum\n",
    "        moving_factor = normalized_row_sum_part[3:5].sum() - normalized_row_sum_part[0:2].sum()\n",
    "        if moving_factor > 0.0:\n",
    "            x_candidate += 1\n",
    "        elif moving_factor < 0.0:\n",
    "            x_candidate -= 1\n",
    "\n",
    "    center_x = x_candidate\n",
    "\n",
    "    if center_x >= gray_eye.shape[1]-2 or center_x <= 2:\n",
    "        center_x = -1\n",
    "    elif center_y >= gray_eye.shape[0]-1 or center_y <= 1:\n",
    "        center_y = -1\n",
    "\n",
    "    return [center_x, center_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫번째, 눈 이미지를 low pass filter로 smoothing, bilateral filter사용  \n",
    "두번째, 1차원 값을 누적하여 y축 기준의 최대값으로 중심점의 y축 좌표를 얻음.  \n",
    "세번째, x축은 최대값 지점에서 mean shift를 적용하여 양 끝단에 수렴하는 예외는 처리한 후 x축 좌표를 얻음.  \n",
    "### 키포인트 검출 딥러닝 모델 만들기\n",
    "#### 데이터 확인\n",
    "학습을 위해  대량의 눈동자 위치 라벨 필요. 10000개 이상의 데이터셋이 필요  \n",
    "LFW데이터셋에 앞의 눈동자 검출 방법을 적용시켜 데이터셋을 생성한다.  \n",
    "cd ~/aiffel/coarse_to_fine && python prepare_eye_dataset.py  \n",
    "tensorflow hub에서 제공하는 pretrained image feature embedding으로 fine tuning  \n",
    "ImageDataGenerator형식으로 데이터를 읽음, 라벨이 image형태로 저장되었음.  \n",
    "train dataset 23712, val dataset 2638  \n",
    "image_generator, label generator는 텐서플로우의 generator형식을 사용하였기에 출력 형식도 맞춘다.  \n",
    "제너레이터 : https://tensorflow.blog/%ED%9A%8C%EC%98%A4%EB%A6%AC%EB%B0%94%EB%9E%8C%EC%9D%84-%ED%83%84-%ED%8C%8C%EC%9D%B4%EC%8D%AC/%EC%A0%9C%EB%84%88%EB%A0%88%EC%9D%B4%ED%84%B0/  \n",
    "학습라벨에는 3개의 점이 lavel이미지에 표시되어 있다.  \n",
    "눈의 왼쪽 끝을 1, 오른쪽 끝을 2, 중심을 3으로 지정하였고 np.where()함수로 이미지에서 좔표로 복원한다.  \n",
    "아래 코드를 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train/input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-28de5ffefa71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mimage_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlabel_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mlabel_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMAGE_SHAPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow_from_directory\u001b[0;34m(self, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0my\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \"\"\"\n\u001b[0;32m--> 524\u001b[0;31m         return DirectoryIterator(\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.8/site-packages/keras_preprocessing/image/directory_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, image_data_generator, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubdir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train/input'"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "list_image = sorted(glob('./data/train/input/img/*.png'))\n",
    "list_label = sorted(glob('./data/train/label/mask/*.png'))\n",
    "print (len(list_image), len(list_label))\n",
    "\n",
    "IMAGE_SHAPE = (80, 120)\n",
    "data_root = './data/train/input'\n",
    "label_root = './data/train/label'\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data = image_generator.flow_from_directory(str(data_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)\n",
    "label_data = label_generator.flow_from_directory(str(label_root), class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)\n",
    "\n",
    "\n",
    "def user_generation(train_generator, label_generator):\n",
    "    h, w = train_generator.target_size\n",
    "    for images, labels in zip(train_generator, label_generator):\n",
    "        images /= 255.\n",
    "        images = images[..., ::-1] # rgb to bgr\n",
    "\n",
    "        list_point_labels = []\n",
    "        for img, label in zip(images, labels):\n",
    "\n",
    "            eye_ls = np.where(label==1) # leftside\n",
    "            eye_rs = np.where(label==2) # rightside\n",
    "            eye_center = np.where(label==3)\n",
    "\n",
    "            lx, ly = [eye_ls[1].mean(), eye_ls[0].mean()]\n",
    "            rx, ry = [eye_rs[1].mean(), eye_rs[0].mean()]\n",
    "            cx, cy = [eye_center[1].mean(), eye_center[0].mean()]\n",
    "\n",
    "            if len(eye_ls[0])==0 or len(eye_ls[1])==0:\n",
    "                lx, ly = [0, 0]\n",
    "            if len(eye_rs[0])==0 or len(eye_rs[1])==0:\n",
    "                rx, ry = [w, h]\n",
    "            if len(eye_center[0])==0 or len(eye_center[1])==0:\n",
    "                cx, cy = [0, 0]\n",
    "\n",
    "            np_point_label = np.array([lx/w,ly/h,rx/w,ry/h,cx/w,cy/h], dtype=np.float32)\n",
    "\n",
    "            list_point_labels.append(np_point_label)\n",
    "        np_point_labels = np.array(list_point_labels)\n",
    "        yield (images, np_point_labels)\n",
    "        \n",
    "user_train_generator = user_generation(image_data, label_data)\n",
    "for i in range(2):\n",
    "    dd = next(user_train_generator)\n",
    "    print (dd[0][0].shape, dd[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 설계\n",
    "tensorflow Hub에서 resnet의 특성추출기 부분을 백본으로 사용  \n",
    "데이터 제너레이터에서 출력을 6개(각 점당 좌표, 2 * 3)로 했기 때문에 num_classes는 6으로 설정  \n",
    "position regression문제이므로 loss와 metric은 mse, mae로 설정한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' tf hub feature_extractor '''\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                            input_shape=(80,120,3))\n",
    "\n",
    "image_batch = next(image_data)\n",
    "feature_batch = feature_extractor_layer(image_batch)\n",
    "print(feature_batch.shape)\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "feature_extractor_layer.trainable = False\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    #layers.Dense(1024, activation='relu'),\n",
    "    #layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  loss='mse',\n",
    "  metrics=['mae']\n",
    "  )\n",
    "\n",
    "def lr_step_decay(epoch):\n",
    "    init_lr = 0.0005 #self.flag.initial_learning_rate\n",
    "    lr_decay = 0.5 #self.flag.learning_rate_decay_factor\n",
    "    epoch_per_decay = 2 #self.flag.epoch_per_decay\n",
    "    lrate = init_lr * math.pow(lr_decay, math.floor((1+epoch)/epoch_per_decay))\n",
    "    return lrate\n",
    "    \n",
    "steps_per_epoch = image_data.samples//image_data.batch_size\n",
    "print (image_data.samples, image_data.batch_size, steps_per_epoch)\n",
    "# 20160 32 630 -> 데이터를 batch_size 의 배수로 준비해 주세요.\n",
    "\n",
    "learning_rate = LearningRateScheduler(lr_step_decay)\n",
    "\n",
    "history = model.fit(user_train_generator, epochs=10,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks = [learning_rate]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SHAPE = (80, 120)\n",
    "val_data_root = './data/val/input'\n",
    "val_label_root = './data/val/label'\n",
    "\n",
    "image_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator_val = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data_val = image_generator.flow_from_directory(str(val_data_root),\n",
    "                                                     class_mode=None,\n",
    "                                                     target_size=IMAGE_SHAPE,\n",
    "                                                     shuffle=False)\n",
    "label_data_val = label_generator.flow_from_directory(str(val_label_root),\n",
    "                                                     class_mode=None,\n",
    "                                                     target_size=IMAGE_SHAPE,\n",
    "                                                     shuffle=False)\n",
    "\n",
    "user_val_generator = user_generation(image_data_val, label_data_val)\n",
    "mse, mae = model.evaluate_generator(user_val_generator, image_data_val.n // 32)\n",
    "print(mse, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평균 에러가 0.013정도 나옴.  \n",
    "120픽셀 기준으로 120 * 0.013= 1.56픽셀 정도 에러 발생  \n",
    "이미지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img test\n",
    "img = cv2.imread('./data/val/input/img/eye_000010_l.png')\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력 파이프에 맞게 이미지 리사이즈(120, 80)후 배치 차원을 추가하고 한 장만 출력하니깐 배치 size를 1로 한다.  \n",
    "출력값은 좌측, 우측, 중앙좌표"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_inputs = np.expand_dims(cv2.resize(img, (120, 80)), axis=0)\n",
    "preds = model.predict(np_inputs/255., 1)\n",
    "\n",
    "repred = preds.reshape((1, 3, 2))\n",
    "repred[:,:,0] *= 120\n",
    "repred[:,:,1] *= 80\n",
    "print (repred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지를 출력하려면 pt에 0.5를 곱해야 한다. 사용하는 데이터 크기가 60x40이므로 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
