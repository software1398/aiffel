{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV12 - 직접 만들어보는 OCR\n",
    "\n",
    "텍스트 detection은 Keras-OCR를 이용하고 recognition은 직접 만들어보기  \n",
    "keras-ocr : https://github.com/faustomorales/keras-ocr  \n",
    "craft : https://arxiv.org/pdf/1904.01941.pdf  \n",
    "https://github.com/notAI-tech/keras-craft  \n",
    "모델의 발전사 : https://arxiv.org/pdf/1904.01906.pdf  \n",
    "### Dataset for OCR\n",
    "Recognition model의 정량적인 평가를 위해 MJ Synth, SynthText 데이터셋을 활용한다.  \n",
    "http://www.robots.ox.ac.uk/~vgg/data/text/   \n",
    "http://www.robots.ox.ac.uk/~vgg/data/scenetext/  \n",
    "MJ데이터  \n",
    "https://drive.google.com/drive/folders/1xYcMdOMdP4N738hBTg6Pg-40vWf6jJz4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir -p ~/aiffel/ocr\n",
    "# cd ~/aiffel/ocr\n",
    "\n",
    "import os\n",
    "\n",
    "path = os.path.join(os.getenv('HOME'),'aiffel/ocr')\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognition model\n",
    "CRNN 구조를 활용해서 만들어보기  \n",
    "https://arxiv.org/pdf/1507.05717.pdf  \n",
    "이미지를 conv를 통해 feature맵 추출하고 recurrent layer에서 전체적인 context을 파악하고 다양한 output의 크기에 대응 가능  \n",
    "transcription layer은 step마다 어떤 character의 확률이 높은지 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of characters is 36\n"
     ]
    }
   ],
   "source": [
    "NUMBERS = \"0123456789\"\n",
    "ENG_CHAR_UPPER = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "TARGET_CHARACTERS = ENG_CHAR_UPPER + NUMBERS\n",
    "print(f\"The total number of characters is {len(TARGET_CHARACTERS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "HOME_DIR = os.getenv('HOME')+'/aiffel/ocr'\n",
    "\n",
    "TRAIN_DATA_PATH = HOME_DIR+'/MJ/MJ_train'\n",
    "VALID_DATA_PATH = HOME_DIR+'/MJ/MJ_valid'\n",
    "TEST_DATA_PATH = HOME_DIR+'/MJ/MJ_test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:72, height:31\n",
      "target_img_size:(74, 32)\n",
      "display img shape:(74, 32, 3)\n",
      "label:Lube\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=74x32 at 0x7F9A02F0BC90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:82, height:31\n",
      "target_img_size:(84, 32)\n",
      "display img shape:(84, 32, 3)\n",
      "label:Spencerian\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=84x32 at 0x7F9A02F0B850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:115, height:31\n",
      "target_img_size:(100, 32)\n",
      "display img shape:(100, 32, 3)\n",
      "label:accommodatingly\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x32 at 0x7F9A02F1F050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image width:140, height:31\n",
      "target_img_size:(100, 32)\n",
      "display img shape:(100, 32, 3)\n",
      "label:CARPENTER\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=100x32 at 0x7F9A02F1F050>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "env = lmdb.open(TRAIN_DATA_PATH, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "with env.begin(write=False) as txn:\n",
    "    for index in range(1, 5):\n",
    "        label_key = 'label-%09d'.encode() % index\n",
    "        label = txn.get(label_key).decode('utf-8')\n",
    "        img_key = 'image-%09d'.encode() % index\n",
    "        imgbuf = txn.get(img_key)\n",
    "        buf = six.BytesIO()\n",
    "        buf.write(imgbuf)\n",
    "        buf.seek(0)\n",
    "        try:\n",
    "            img = Image.open(buf).convert('RGB')\n",
    "\n",
    "        except IOError:\n",
    "            img = Image.new('RGB', (100, 32))\n",
    "            label = '-'\n",
    "        width, height = img.size\n",
    "        print('original image width:{}, height:{}'.format(width, height))\n",
    "        \n",
    "        target_width = min(int(width*32/height), 100)\n",
    "        target_img_size = (target_width,32 )\n",
    "        \n",
    "        print('target_img_size:{}'.format(target_img_size))\n",
    "        \n",
    "        img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "       \n",
    "        print('display img shape:{}'.format(img.shape))\n",
    "        print('label:{}'.format(label))\n",
    "        display(Image.fromarray(img.transpose(1,0,2).astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지의 높이는 31, 32이고, 너비는 문자열 길이에 따라 다양하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MJDatasetSequence(Sequence):\n",
    "    def __init__(self, \n",
    "                      dataset_path,\n",
    "                      label_converter,\n",
    "                      batch_size=1,\n",
    "                      img_size=(100,32),\n",
    "                      max_text_len=22,\n",
    "                      is_train=False,\n",
    "                      character=''\n",
    "                ):\n",
    "        \n",
    "        self.label_converter = label_converter\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.max_text_len = max_text_len\n",
    "        self.character = character\n",
    "        self.is_train = is_train\n",
    "        self.divide_length = 100\n",
    "\n",
    "        self.env = lmdb.open(dataset_path, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            num_samples = int(txn.get('num-samples'.encode()))\n",
    "            self.num_samples = int(num_samples)\n",
    "            self.index_list = [index + 1 for index in range(self.num_samples)]\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "        return math.ceil(self.num_samples/self.batch_size/self.divide_length)\n",
    "    \n",
    "    def _get_img_label(self, index):\n",
    "        # Return image array and label in string\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "                img = Image.open(buf).convert('RGB')\n",
    "\n",
    "            except IOError:\n",
    "                img = Image.new('RGB', self.img_size)\n",
    "                label = '-'\n",
    "            width, height = img.size\n",
    "            \n",
    "            target_width = min(int(width*self.img_size[1]/height), self.img_size[0])\n",
    "            target_img_size = (target_width,self.img_size[1] )\n",
    "            img = np.array(img.resize(target_img_size)).transpose(1,0,2)\n",
    "            label = label.upper()[:self.max_text_len]\n",
    "            out_of_char = f'[^{self.character}]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_indicies = self.index_list[\n",
    "            idx*self.batch_size:\n",
    "            (idx+1)*self.batch_size\n",
    "        ]\n",
    "        input_images = np.zeros([self.batch_size, *self.img_size, 3])\n",
    "        labels = np.zeros([self.batch_size, self.max_text_len], dtype='int64')\n",
    "\n",
    "        input_length = np.ones([self.batch_size], dtype='int64')*self.max_text_len\n",
    "        label_length = np.ones([self.batch_size], dtype='int64')\n",
    "\n",
    "        for i, index in enumerate(batch_indicies):\n",
    "            img, label = self._get_img_label(index)\n",
    "            encoded_label = self.label_converter.encode(label)\n",
    "            width = img.shape[0]\n",
    "            input_images[i,:width,:,:] = img\n",
    "            if len(encoded_label) > self.max_text_len:\n",
    "                continue\n",
    "            labels[i,0:len(encoded_label)] = encoded_label\n",
    "            label_length[i] = len(encoded_label)\n",
    "\n",
    "        inputs = {\n",
    "            'input_image': input_images,\n",
    "            'label': labels,\n",
    "            'input_length': input_length,\n",
    "            'label_length': label_length,\n",
    "        }\n",
    "        outputs = {'ctc': np.zeros([self.batch_size, 1])}\n",
    "        return inputs, outputs\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        self.index_list =  [index + 1 for index in range(self.num_samples)]\n",
    "        if self.is_train :\n",
    "            np.random.shuffle(self.index_list)\n",
    "            return self.index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 데이터를 img, label의 쌍으로 가져온다. \n",
    "### 3) Encode\n",
    "각 character들이 어떤 index와 매핑되어 저장된다. 모델이 학습할 수 있게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelConverter(object):\n",
    "    \n",
    "     \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        self.character = \"-\" + character\n",
    "        self.label_map = dict()\n",
    "        for i, char in enumerate(self.character):\n",
    "            self.label_map[char] = i\n",
    "\n",
    "    def encode(self, text):\n",
    "        encoded_label = []\n",
    "        for i, char in enumerate(text):\n",
    "            if i > 0 and char == text[i - 1]:\n",
    "                encoded_label.append(0)\n",
    "            encoded_label.append(self.label_map[char])\n",
    "        return np.array(encoded_label)\n",
    "\n",
    "    def decode(self, encoded_label):\n",
    "        target_characters = list(self.character)\n",
    "        decoded_label = \"\"\n",
    "        for encode in encoded_label:\n",
    "            decoded_label += self.character[encode]\n",
    "        return decoded_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encdoded_text:  [ 8  5 12  0 12 15]\n",
      "Decoded_text:  HEL-LO\n"
     ]
    }
   ],
   "source": [
    "label_converter = LabelConverter(TARGET_CHARACTERS)\n",
    "\n",
    "encdoded_text = label_converter.encode('HELLO')\n",
    "print(\"Encdoded_text: \", encdoded_text)\n",
    "decoded_text = label_converter.decode(encdoded_text)\n",
    "print(\"Decoded_text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Build CRNN model\n",
    "K.ctc_batch_cost()를 활용해서 loss계산하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args): # CTC loss를 계산하기 위한 Lambda 함수\n",
    "    labels, y_pred, label_length, input_length = args\n",
    "    y_pred = y_pred[:, 2:, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input과 lable의 길이가 다르다.  \n",
    "input이 AAAPPPPLLLLEE이고 label이 APPLE일 경우가 있다.  \n",
    "text recognition과 유사한 경우다.  \n",
    "K.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
    "y_ture: 실제 라벨, 핫인코딩된 형태, max_string_length=22  \n",
    "y_pred: RCNN모델의 출력 결과, 24  \n",
    "input_length: 모델 입력 길이 T, 이미지 width\n",
    "label_length: 실제 정답 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_input을 입력하고 label를 output으로 출력하는 함수\n",
    "\n",
    "def build_crnn_model(input_shape=(100,32,3), characters=TARGET_CHARACTERS):\n",
    "    num_chars = len(characters)+2\n",
    "    image_input = layers.Input(shape=input_shape, dtype='float32', name='input_image')\n",
    "    \n",
    "    # Build CRNN model\n",
    "    conv = layers.Conv2D(64, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(image_input)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.Conv2D(256, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.BatchNormalization()(conv)\n",
    "    conv = layers.Conv2D(512, (3, 3), activation='relu', padding='same', kernel_initializer='he_normal')(conv)\n",
    "    conv = layers.MaxPooling2D(pool_size=(1, 2))(conv)     \n",
    "    feature = layers.Conv2D(512, (2, 2), activation='relu', kernel_initializer='he_normal')(conv)\n",
    "    sequnce = layers.Reshape(target_shape=(24, 512))(feature)\n",
    "    sequnce = layers.Dense(64, activation='relu')(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    sequnce = layers.Bidirectional(layers.LSTM(256, return_sequences=True))(sequnce)\n",
    "    y_pred = layers.Dense(num_chars, activation='softmax', name='output')(sequnce)\n",
    "\n",
    "    labels = layers.Input(shape=[22], dtype='int64', name='label')\n",
    "    input_length = layers.Input(shape=[1], dtype='int64', name='input_length')\n",
    "    label_length = layers.Input(shape=[1], dtype='int64', name='label_length')\n",
    "    loss_out = layers.Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")(\n",
    "        [labels, y_pred, label_length, input_length]\n",
    "    )\n",
    "    model_input = [image_input, labels, input_length, label_length]\n",
    "    model = Model(\n",
    "        inputs=model_input,\n",
    "        outputs=loss_out\n",
    "    )\n",
    "    y_func = tf.keras.backend.function(image_input, [y_pred])\n",
    "    return model, y_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Train & Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = MJDatasetSequence(TRAIN_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS, is_train=True)\n",
    "val_set = MJDatasetSequence(VALID_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "\n",
    "checkpoint_path = HOME_DIR + '/model_checkpoint.hdf5'\n",
    "\n",
    "model, y_func = build_crnn_model()\n",
    "sgd = tf.keras.optimizers.Adadelta(lr=0.1, clipnorm=5)\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n",
    "ckp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, monitor='val_loss',\n",
    "    verbose=1, save_best_only=True, save_weights_only=True\n",
    ")\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'\n",
    ")\n",
    "model.fit(train_set,\n",
    "        steps_per_epoch=len(val_set),\n",
    "        epochs=100,\n",
    "        validation_data=val_set,\n",
    "        validation_steps=len(val_set),\n",
    "        callbacks=[ckp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "test_set = MJDatasetSequence(TEST_DATA_PATH, label_converter, batch_size=BATCH_SIZE, character=TARGET_CHARACTERS)\n",
    "\n",
    "model, y_func = build_crnn_model()\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "input_data = model.get_layer('input_image').output\n",
    "y_pred = model.get_layer('output').output\n",
    "model_pred = Model(inputs=input_data, outputs=y_pred)\n",
    "\n",
    "def decode_predict_ctc(out, chars = TARGET_CHARACTERS, top_paths = 1):\n",
    "    results = []\n",
    "    beam_width = 5\n",
    "    if beam_width < top_paths:\n",
    "        beam_width = top_paths\n",
    "    for i in range(top_paths):\n",
    "        indexes = K.get_value(\n",
    "            K.ctc_decode(\n",
    "                out, input_length = np.ones(out.shape[0]) * out.shape[1],\n",
    "                greedy =False , beam_width = beam_width, top_paths = top_paths\n",
    "            )[0][i]\n",
    "        )[0]\n",
    "        text = \"\"\n",
    "        for index in indexes:\n",
    "            text += chars[index]\n",
    "        results.append(text)\n",
    "    return results\n",
    "\n",
    "def check_inference(model, dataset, index = 5):\n",
    "    for i in range(index):\n",
    "        inputs, outputs = dataset[i]\n",
    "        img = dataset[i][0]['input_image'][0:1,:,:,:]\n",
    "        output = model_pred.predict(img)\n",
    "        result = decode_predict_ctc(output, chars=\"-\"+TARGET_CHARACTERS)[0].replace('-','')\n",
    "        print(\"Result: \\t\", result)\n",
    "        display(Image.fromarray(img[0].transpose(1,0,2).astype(np.uint8)))\n",
    "\n",
    "check_inference(model, test_set, index=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
