{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV10 - Class Activation Map\n",
    "### CAM, Grad-CAM용 모델 준비하기\n",
    "1. 데이터셋 준비하기  \n",
    "   CAM은 CNN뒤에 GAP과 소프트맥스 레이어가 붙는다.  \n",
    "   Grad-CAM은 이런 제약이 없다.  \n",
    "   CAM은 클래스에 대한 활성화 정도를 나타낸 지도.  \n",
    "   분류 모델이 기본이다.  \n",
    "   목표는 이미지 내에 클래스가 활성화된 위치를 찾는 것이므로 위치정보도 필요하다  \n",
    "   tensorflow datasets에서 Cars196 데이터셋을 사용  \n",
    "   이 데이터셋에서 라벨에 위치정보인 바운딩 박스 정보를 포함하고 있다.  \n",
    "   https://www.tensorflow.org/datasets/catalog/cars196  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import copy\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset cars196/2.0.0 (download: 1.82 GiB, generated: Unknown size, total: 1.82 GiB) to /home/aiffel0042/tensorflow_datasets/cars196/2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f808c453574e79aede5321de49889b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ce1ccfd3ee412eb3261ea9c3cad510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0eefadbbba49d583fba92d2863bd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Extraction completed...', max=1.0, styl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 최초 수행시에는 다운로드가 진행됩니다. 오래 걸릴 수 있으니 유의해 주세요.  \n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'cars196',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.show_examples(ds_test, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 물체의 위치정보  \n",
    "   bbox를 표시하는 방법들  \n",
    "   'xywh'와 'minmax'를 주로 사용한다.  \n",
    "   'xywh'는 바운딩박스의 중심점을 x, y로 표기하고 너비와 높이를 w, h로 표기한다.  \n",
    "   x, y는 좌측상단 좌표일 수 있다.  \n",
    "   'minmax'는 바운딩박스 좌표의 최소값과 최대값을 표기한다.(x_min, x_max, y_min, y_max)  \n",
    "   전체 이미지의 너비와 높이를 기준으로 normalize한 상대적인 값을 표기한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. CAM을 위한 모델 만들기  \n",
    "   이미지넷 데이터로 훈련된 resnet50을 기반으로 하고 pooling layer뒤에 소프트맥스 레이어를 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "base_model = keras.applications.resnet.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224,3),\n",
    "    pooling='avg',\n",
    ")\n",
    "x = base_model.output\n",
    "preds = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "cam_model=keras.Model(inputs=base_model.input, outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. CAM 모델 학습하기  \n",
    "   bbox는 CAM모델 학습에 필요하지 않으므로 제외한다.  \n",
    "   CAM모델은 object detection이나 segmentation에 활용될 수 있지만,  \n",
    "   바운딩 박스를 직접 사용하지 않고 weakly supervised learning을 통해 물체 영역을 간접적으로 학습시키는 방식이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img(input):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.image.resize(input['image'], [224, 224])\n",
    "    input['image'] = tf.cast(image, tf.float32) / 255.\n",
    "    return input['image'], input['label']\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=2\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_norm = apply_normalize_on_dataset(ds_train)\n",
    "ds_test_norm = apply_normalize_on_dataset(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(2020)\n",
    "cam_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(lr=0.01),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cam_model = cam_model.fit(\n",
    "    ds_train_norm,\n",
    "    steps_per_epoch=int(ds_info.splits['train'].num_examples/16),\n",
    "    validation_steps=int(ds_info.splits['test'].num_examples/16),\n",
    "    epochs=15,\n",
    "    validation_data=ds_test_norm,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cam_model_path = os.getenv('HOME')+'/aiffel/class_activation_map/cam_model.h5'\n",
    "cam_model.save(cam_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 장을 뽑는 함수\n",
    "\n",
    "def get_one(ds):\n",
    "    ds = ds.take(1)\n",
    "    sample_data = list(ds.as_numpy_iterator())\n",
    "    bbox = sample_data[0]['bbox']\n",
    "    image = sample_data[0]['image']\n",
    "    label = sample_data[0]['label']\n",
    "    return sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_model = tf.keras.models.load_model(cam_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAM을 만들기 위해서는 특성맵, 클래스 별 확률을 얻기 위한 소프트맥스 레이어의 가중치, 원하는 클래스의 출력값이 필요하다.  \n",
    "그리고 이미지에서 모델이 어떤 부분을 보는지 시각화하려면 입력 이미지 사이즈와 같게 만들어야 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    \n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # 학습한 모델에서 원하는 Layer의 output을 얻기 위해서 모델의 input과 output을 새롭게 정의해줍니다.\n",
    "    # model.layers[-3].output에서는 우리가 필요로 하는 GAP 이전 Convolution layer의 output을 얻을 수 있습니다.\n",
    "    cam_model = tf.keras.models.Model([model.inputs], [model.layers[-3].output, model.output])\n",
    "    conv_outputs, predictions = cam_model(tf.expand_dims(img_tensor, 0))\n",
    "    \n",
    "    conv_outputs = conv_outputs[0, :, :, :]\n",
    "    class_weights = model.layers[-1].get_weights()[0] #마지막 모델의 weight activation을 가져옵니다.\n",
    "    \n",
    "    cam_image = np.zeros(dtype=np.float32, shape=conv_outputs.shape[0:2])\n",
    "    for i, w in enumerate(class_weights[:, class_idx]):\n",
    "        # W * f 를 통해 class별 activation map을 계산합니다.\n",
    "        cam_image += w * conv_outputs[:, :, i]\n",
    "\n",
    "    cam_image /= np.max(cam_image) # activation score를 normalize합니다.\n",
    "    cam_image = cam_image.numpy()\n",
    "    cam_image = cv2.resize(cam_image, (width, height)) # 원래 이미지의 크기로 resize합니다.\n",
    "    return cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAM 이미지와 원본 이미지를 겹쳐보자.\n",
    "def visualize_cam_on_image(src1, src2, alpha=0.5):\n",
    "    beta = (1.0 - alpha)\n",
    "    merged_image = cv2.addWeighted(src1, alpha, src2, beta, 0.0)\n",
    "    return merged_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_image = item['image'].astype(np.uint8)\n",
    "cam_image_3channel = np.stack([cam_image*255]*3, axis=-1).astype(np.uint8)\n",
    "\n",
    "blended_image = visualize_cam_on_image(cam_image_3channel, origin_image)\n",
    "plt.imshow(blended_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model, activation_layer, item):\n",
    "    item = copy.deepcopy(item)\n",
    "    width = item['image'].shape[1]\n",
    "    height = item['image'].shape[0]\n",
    "    img_tensor, class_idx = normalize_and_resize_img(item)\n",
    "    \n",
    "    # Grad cam에서도 cam과 같이 특정 레이어의 output을 필요로 하므로 모델의 input과 output을 새롭게 정의합니다.\n",
    "    # 이때 원하는 레이어가 다를 수 있으니 해당 레이어의 이름으로 찾은 후 output으로 추가합니다.\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(activation_layer).output, model.output])\n",
    "    \n",
    "    # Gradient를 얻기 위해 tape를 사용합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, pred = grad_model(tf.expand_dims(img_tensor, 0))\n",
    "    \n",
    "        loss = pred[:, class_idx] # 원하는 class(여기서는 정답으로 활용) 예측값을 얻습니다.\n",
    "        output = conv_output[0] # 원하는 layer의 output을 얻습니다.\n",
    "        grad_val = tape.gradient(loss, conv_output)[0] # 예측값에 따른 Layer의 gradient를 얻습니다.\n",
    "\n",
    "    weights = np.mean(grad_val, axis=(0, 1)) # gradient의 GAP으로 class별 weight를 구합니다.\n",
    "    grad_cam_image = np.zeros(dtype=np.float32, shape=conv_output.shape[0:2])\n",
    "    for k, w in enumerate(weights):\n",
    "        # 각 class별 weight와 해당 layer의 output을 곱해 class activation map을 얻습니다.\n",
    "        grad_cam_image += w * output[:, :, k]\n",
    "        \n",
    "    grad_cam_image /= np.max(grad_cam_image)\n",
    "    grad_cam_image = grad_cam_image.numpy()\n",
    "    grad_cam_image = cv2.resize(grad_cam_image, (width, height))\n",
    "    return grad_cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv5_block3_out', item)\n",
    "plt.imshow(grad_cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv4_block3_out', item)\n",
    "plt.imshow(grad_cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_cam_image = generate_grad_cam(cam_model, 'conv3_block3_out', item)\n",
    "plt.imshow(grad_cam_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detection with CAM\n",
    "CAM에서 물체의 위치를 찾는 detection해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = get_one(ds_test)\n",
    "print(item['label'])\n",
    "plt.imshow(item['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_image = generate_cam(cam_model, item)\n",
    "plt.imshow(cam_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바운딩 박스를 만드는 함수\n",
    "# score_thresh: 역치값 이하의 바운딩 박스는 삭제\n",
    "# OpenCV의 findcontours(), minAreaRect()를 이용하여 사각형 찾기\n",
    "# rotated_rect라는 회전된 라운딩 박스 얻음.\n",
    "# boxPoints()을 이용하여 꼭지점으로 바꿔줌.\n",
    "\n",
    "def get_bbox(cam_image, score_thresh=0.05):\n",
    "    low_indicies = cam_image <= score_thresh\n",
    "    cam_image[low_indicies] = 0\n",
    "    cam_image = (cam_image*255).astype(np.uint8)\n",
    "    \n",
    "    contours,_ = cv2.findContours(cam_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    rotated_rect = cv2.minAreaRect(cnt)\n",
    "    rect = cv2.boxPoints(rotated_rect)\n",
    "    rect = np.int0(rect)\n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = copy.deepcopy(item['image'])\n",
    "rect = get_bbox(cam_image)\n",
    "rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.drawContours(image,[rect],0,(0,0,255),2)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intersection Over Union  \n",
    "모델에 의해 도출된 바운딩 박스와 정답 데이터의 바운딩 박스를 비교하기  \n",
    "IoU: 두 개 영역의 합집합인 Union으로 교집합 영역인 intersection으로 나누어준 값  \n",
    "https://www.youtube.com/watch?v=ANIzQ5G-XPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rect의 좌표는 (x, y) 형태로, bbox는 (y_min, x_min, y_max, x_max)의 normalized 형태로 주어집니다. \n",
    "def rect_to_minmax(rect, image):\n",
    "    bbox = [\n",
    "        rect[:,1].min()/float(image.shape[0]),  #bounding box의 y_min\n",
    "        rect[:,0].min()/float(image.shape[1]),  #bounding box의 x_min\n",
    "        rect[:,1].max()/float(image.shape[0]),  #bounding box의 y_max\n",
    "        rect[:,0].max()/float(image.shape[1])   #bounding box의 x_max\n",
    "    ]\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bbox = rect_to_minmax(rect, item['image'])\n",
    "pred_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth bbox\n",
    "item['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(boxA, boxB):\n",
    "    y_min = max(boxA[0], boxB[0])\n",
    "    x_min= max(boxA[1], boxB[1])\n",
    "    y_max = min(boxA[2], boxB[2])\n",
    "    x_max = min(boxA[3], boxB[3])\n",
    "    \n",
    "    interArea = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_iou(pred_bbox, item['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
