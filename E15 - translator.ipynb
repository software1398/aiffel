{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E15 - 번역기를 만들어보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계 번역에 대해   \n",
    "규칙 기반의 기계 번역 -> 통계적 기계 번역 -> 신경망 기계 번역(2016년, 구글 번역기)   \n",
    "\n",
    "RNN   \n",
    "1. one-to-one: 일반적인 피드 포워드 신경망   \n",
    "2. one-to-many: 하나의 입력으로부터 다수의 출력, image captioning   \n",
    "3. many-to-one: 여러 입력으로부터 하나의 결과 출력, 텍스트 분류, 스펨 메일 분류기    \n",
    "4. many-to-many: 5번과 달리 어떤 스텝에서 출력이 시작, 기계 번역기, 문장을 모두 읽은 후에야 번역이 가능    \n",
    "5. many-to-many: 매 스텝마다 바로 출력, 입력 문장의 각 단어가 무엇인지, 개체명 인식, 품사 태깅에 사용   \n",
    "\n",
    "seq2seq 구조(번역기의 기본 구조)   \n",
    "두 개의 RNN을 연결한 구조, 입력 문장을 받는 RNN(인코더), 그 다음 RNN(디코더)   \n",
    "인코더에서 어떤 데이터 X에서 저차원의 feature vector z를 만들어 낸다.    \n",
    "디코더는 저차원의 feature vector z로부터 정보를 복원하여 다시 어떤 데이터 X'을 재생성한다.   \n",
    "\n",
    "인코더 RNN은 입력문장을 해석해서 만든 hidden state 벡터.   \n",
    "A언어의 문장 X를 z라는 hidden state로 해석한 후 z를 다시 B언어의 문장 Y로 재생성하는 구조.\n",
    "디코더 RNN은 인코더 RNN의 마지막 스텝의 hidden state를 전달받는다.    \n",
    "이 hidden state를 토대로 출력 문장을 만들어 낸다.   \n",
    "_GO: 출력 시작을 의미하는 단어, EOS: 출력 종료 단어   \n",
    "\n",
    "조건적 언어 모델(conditional language model): 조건에 맞는 문장을 생성하라. 기계 번역   \n",
    "X라는 영어 문장을 Y라는 프랑스어 문장을 만들기, 프랑스어라는 조건을 붙여준다.   \n",
    "즉, X -> c -> Y, RNN-RNN-RNN -> seq2seq   \n",
    "\n",
    "교사 강요(teacher forcing)   \n",
    "seq2seq는 훈련 과정과 테스트 과정이 있는데 테스트 과정에서 문장을 만들어낸다.   \n",
    "훈련 과정에서 잘못된 hidden state를 넘겨준다면 테스트 과정에서 잘못된 문장을 만들어 낼 것이다.\n",
    "훈련 과정에서는 정답을 알고 있는 상황이므로 이전 time step의 예측값이 아닌 실제값을 입력으로 사용할 수 있다.   \n",
    "이를 교사 강요라 하며 실제 sequence 데이터의 생성 모델에서 사용하는 기법이다.   \n",
    "훈련 데이터 이외의 결과를 생성해내는 능력을 기르는 데에는 방해가 될 수 있다.   \n",
    "\n",
    "seq2seq는 단어 수준 또는 문자 수준으로 입출력 단위를 정할 수 있다.   \n",
    "문자 수준의 출력층은 알파펫의 경우 대소문자 구분하지 않고 특수문자를 포함하더라도 100을 넘지 않는다.   \n",
    "어떤 수준이 번역에 유리할까? 둘은 trade-off관계다.\n",
    "단어 수준의 번역은 단어의 변화가 셀 수 없을 정도로 많고 띄어쓰기가 있을 경우 전처리가 어렵다.   \n",
    "문자 수준의 번역은 너무 작게 쪼개져서 단어 안에 내재된 정보가 소실된다. 단어를 이루는 패턴을 학습해야 한다.   \n",
    "최신 자연어 처리 흐름은 subword기반의 번역이 주를 이룬다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 사용할 데이터: https://www.manythings.org/anki/   \n",
    " \n",
    " mkdir -p ~/aiffel/translator_seq2seq/data   \n",
    " mkdir -p ~/aiffel/translator_seq2seq/models   \n",
    " wget https://www.manythings.org/anki/fra-eng.zip   \n",
    " mv fra-eng.zip  ~/aiffel/translator_seq2seq/data   \n",
    " cd ~/aiffel/translator_seq2seq/data && unzip fra-eng.zip   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 178009\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58739</th>\n",
       "      <td>We don't have any proof.</td>\n",
       "      <td>Nous ne disposons d'aucune preuve.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88182</th>\n",
       "      <td>What will the neighbors say?</td>\n",
       "      <td>Que diront les voisins ?</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91472</th>\n",
       "      <td>I read the story three times.</td>\n",
       "      <td>J'ai lu l'histoire trois fois.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71877</th>\n",
       "      <td>She has about 2,000 books.</td>\n",
       "      <td>Elle a à peu près 2000 livres.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170552</th>\n",
       "      <td>Tom doesn't know the difference between linen ...</td>\n",
       "      <td>Tom ne fait pas la différence entre le lin et ...</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "58739                            We don't have any proof.   \n",
       "88182                        What will the neighbors say?   \n",
       "91472                       I read the story three times.   \n",
       "71877                          She has about 2,000 books.   \n",
       "170552  Tom doesn't know the difference between linen ...   \n",
       "\n",
       "                                                      fra  \\\n",
       "58739                  Nous ne disposons d'aucune preuve.   \n",
       "88182                            Que diront les voisins ?   \n",
       "91472                      J'ai lu l'histoire trois fois.   \n",
       "71877                      Elle a à peu près 2000 livres.   \n",
       "170552  Tom ne fait pas la différence entre le lin et ...   \n",
       "\n",
       "                                                       cc  \n",
       "58739   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "88182   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "91472   CC-BY 2.0 (France) Attribution: tatoeba.org #3...  \n",
       "71877   CC-BY 2.0 (France) Attribution: tatoeba.org #1...  \n",
       "170552  CC-BY 2.0 (France) Attribution: tatoeba.org #6...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME')+'/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22603</th>\n",
       "      <td>You won't be shot.</td>\n",
       "      <td>Vous ne serez pas fusillés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11002</th>\n",
       "      <td>I hate that guy.</td>\n",
       "      <td>Je déteste ce mec.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9285</th>\n",
       "      <td>They were dead.</td>\n",
       "      <td>Elles étaient mortes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11468</th>\n",
       "      <td>I wish you luck.</td>\n",
       "      <td>Je vous souhaite bonne chance.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12997</th>\n",
       "      <td>We must succeed.</td>\n",
       "      <td>Il nous faut réussir.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      eng                             fra\n",
       "22603  You won't be shot.     Vous ne serez pas fusillés.\n",
       "11002    I hate that guy.              Je déteste ce mec.\n",
       "9285      They were dead.           Elles étaient mortes.\n",
       "11468    I wish you luck.  Je vous souhaite bonne chance.\n",
       "12997    We must succeed.           Il nous faut réussir."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000] # 5만개 샘플 사용, 세 번째 열은 제외\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31062</th>\n",
       "      <td>Let me pay my share.</td>\n",
       "      <td>\\t Laisse-moi payer ma part. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4619</th>\n",
       "      <td>We must obey.</td>\n",
       "      <td>\\t Il nous faut obéir. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28287</th>\n",
       "      <td>Are you ready to go?</td>\n",
       "      <td>\\t Êtes-vous prête à partir ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>I'll cancel it.</td>\n",
       "      <td>\\t Je l'annulerai. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8098</th>\n",
       "      <td>I like almonds.</td>\n",
       "      <td>\\t J'aime les amandes. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                               fra\n",
       "31062  Let me pay my share.   \\t Laisse-moi payer ma part. \\n\n",
       "4619          We must obey.         \\t Il nous faut obéir. \\n\n",
       "28287  Are you ready to go?  \\t Êtes-vous prête à partir ? \\n\n",
       "8403        I'll cancel it.             \\t Je l'annulerai. \\n\n",
       "8098        I like almonds.         \\t J'aime les amandes. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [10, 5, 8], [10, 5, 8]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어장을 만들기, 영어와 프랑스어별로\n",
    "\n",
    "eng_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "eng_tokenizer.fit_on_texts(lines.eng)               # 50000개의 행을 가진 eng의 각 행에 토큰화를 수행\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng)    # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[11, 1, 19, 4, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 1, 33, 1, 12],\n",
       " [11, 1, 3, 4, 13, 7, 5, 14, 1, 12]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)   # 문자 단위로 Tokenizer를 생성합니다. \n",
    "fra_tokenizer.fit_on_texts(lines.fra)                 # 50000개의 행을 가진 fra의 각 행에 토큰화를 수행\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)     # 단어를 숫자값 인덱스로 변환하여 저장\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) + 1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)\n",
    "# 패딩처리하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 51\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 23\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더인 영어 시퀀스와 달리 프랑스어 시퀸스는 디코더 출려과 비교할 정답 데이터로서 하나와\n",
    "# 교사 강요을 위해 디코더 입력으로서 하나로 두 개를 만든다.\n",
    "# 입력 시퀀스는 <eos>토큰은 필요 없고 비교할 시퀀스는 <sos>토큰이 필요 없다.\n",
    "\n",
    "encoder_input = input_text\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[ char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text] \n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 1, 19, 4, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 1, 33, 1], [11, 1, 3, 4, 13, 7, 5, 14, 1]]\n",
      "[[1, 19, 4, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 1, 33, 1, 12], [1, 3, 4, 13, 7, 5, 14, 1, 12]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])\n",
    "\n",
    "# 12: <eos>토큰, 11: <sos>토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "# 패딩 처리하기\n",
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19  3  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 23, 51)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "# 학습데이터와 검증데이터 분리하기\n",
    "\n",
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련하기\n",
    "\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 인코더 설계\n",
    "# 입력 텐서 생성, 입력 문장을 저장할 변수 텐서\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "# 디코더로 전달할 hidden state, cell state를 리턴. encoder_outputs은 여기서는 불필요.\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도 저장.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# 디코더 설계\n",
    "# 입력 텐서 생성.\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 51)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 315392      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 672,073\n",
      "Trainable params: 672,073\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 6s 17ms/step - loss: 0.9450 - val_loss: 0.8151\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.5745 - val_loss: 0.6531\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.4737 - val_loss: 0.5656\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.4138 - val_loss: 0.5167\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.3750 - val_loss: 0.4816\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.3471 - val_loss: 0.4561\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.3255 - val_loss: 0.4366\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.3087 - val_loss: 0.4256\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2948 - val_loss: 0.4115\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2833 - val_loss: 0.4031\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2734 - val_loss: 0.3896\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2650 - val_loss: 0.3885\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2575 - val_loss: 0.3820\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2506 - val_loss: 0.3778\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2445 - val_loss: 0.3759\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2388 - val_loss: 0.3750\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2335 - val_loss: 0.3735\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2287 - val_loss: 0.3681\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2241 - val_loss: 0.3699\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2199 - val_loss: 0.3671\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2158 - val_loss: 0.3629\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2120 - val_loss: 0.3687\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2083 - val_loss: 0.3590\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2048 - val_loss: 0.3618\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.2015 - val_loss: 0.3669\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1983 - val_loss: 0.3670\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1953 - val_loss: 0.3669\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1923 - val_loss: 0.3659\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1896 - val_loss: 0.3633\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1868 - val_loss: 0.3684\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1843 - val_loss: 0.3694\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1816 - val_loss: 0.3733\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1794 - val_loss: 0.3728\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1768 - val_loss: 0.3706\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1746 - val_loss: 0.3745\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1724 - val_loss: 0.3791\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1703 - val_loss: 0.3788\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1682 - val_loss: 0.3813\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1663 - val_loss: 0.3823\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1644 - val_loss: 0.3870\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1624 - val_loss: 0.3886\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1605 - val_loss: 0.3902\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1588 - val_loss: 0.3922\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1571 - val_loss: 0.3938\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1554 - val_loss: 0.3906\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1536 - val_loss: 0.3931\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1521 - val_loss: 0.4007\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1507 - val_loss: 0.4008\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1490 - val_loss: 0.4064\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 6s 16ms/step - loss: 0.1476 - val_loss: 0.4068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f70c7fa2f90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 51)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 315392    \n",
      "=================================================================\n",
      "Total params: 315,392\n",
      "Trainable params: 315,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 테스트하기\n",
    "# 훈련과 달리 디코더에서 토큰이 '\\n'이 나올 때까지 예측하면서 반복한다.\n",
    "\n",
    "# 인코더 정의\n",
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 정의\n",
    "\n",
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기 상태로 사용.\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현.\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장.\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 356,681\n",
      "Trainable params: 356,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 출력층\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수->단어, 단어->정수\n",
    "\n",
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode_sequence(): 입력 인자는 번역하고자 하는 문장의 정수 시퀀스\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  couriez ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis partie. \n",
      "번역기가 번역한 문장:  je suis partie. \n",
      "-----------------------------------\n",
      "입력 문장: Call us.\n",
      "정답 문장:  Appelez-nous ! \n",
      "번역기가 번역한 문장:  appelez ! \n",
      "-----------------------------------\n",
      "입력 문장: How nice!\n",
      "정답 문장:  Comme c'est gentil ! \n",
      "번역기가 번역한 문장:  comme c'est chaud ! \n",
      "-----------------------------------\n",
      "입력 문장: Turn left.\n",
      "정답 문장:  Tourne à gauche. \n",
      "번역기가 번역한 문장:  fieus-le à cela. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스 (자유롭게 선택해 보세요)\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
